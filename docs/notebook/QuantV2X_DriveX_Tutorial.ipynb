{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuantV2X: 3-Stage Training & PTQ Deep Dive\n",
    "\n",
    "**A comprehensive guide to the 3-stage training pipeline and Post-Training Quantization (PTQ) with detailed code structure and implementation**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Table of Contents\n",
    "\n",
    "### [Part I: Introduction & Code Structure](#part-i)\n",
    "1. [Overview & Setup](#overview)\n",
    "2. [Code Structure Roadmap](#roadmap)\n",
    "3. [Key Components & Architecture](#architecture)\n",
    "\n",
    "### [Part II: 3-Stage Training Pipeline (Detailed)](#part-ii)\n",
    "4. [Stage 1: Full-Precision Pretraining](#stage1)\n",
    "   - Configuration & Code\n",
    "   - Training Script Deep Dive\n",
    "   - Model Architecture\n",
    "   - Training Commands\n",
    "5. [Stage 2: Codebook-Only Training](#stage2)\n",
    "   - Configuration Differences\n",
    "   - Codebook Implementation\n",
    "   - Parameter Freezing Mechanism\n",
    "   - Training Commands\n",
    "6. [Stage 3: End-to-End Co-Training](#stage3)\n",
    "   - Purpose & Key Differences\n",
    "   - Unfreezing Logic\n",
    "   - Training Commands\n",
    "\n",
    "### [Part III: Post-Training Quantization (PTQ)](#part-iii)\n",
    "7. [PTQ Theory & Implementation](#ptq-theory)\n",
    "8. [Quantization Pipeline](#ptq-pipeline)\n",
    "   - PTQ Workflow\n",
    "   - PTQ Commands & Parameters\n",
    "   - Implementation Code\n",
    "9. [Detailed PTQ Workflow](#ptq-workflow)\n",
    "   - Complete Structure of `inference_mc_quant.py`\n",
    "   - Step-by-Step Process\n",
    "10. [Model Structure Comparison](#model-comparison)\n",
    "    - FP32 vs Quantized Models\n",
    "    - Model Printing with `ic()`\n",
    "    - Key Observations\n",
    "11. [QuantModel Implementation](#quantmodel-impl)\n",
    "    - Class Structure\n",
    "    - Layer Replacement Process\n",
    "    - Key Methods\n",
    "\n",
    "12. [Summary & Resources](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part-i'></a>\n",
    "# Part I: Introduction & Code Structure\n",
    "\n",
    "<a id='overview'></a>\n",
    "## 1. Overview & Setup\n",
    "\n",
    "### QuantV2X System Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    QuantV2X Complete Pipeline                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  STAGE 1: Full-Precision Pretraining (20 epochs)               â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â€¢ Model: HeterPyramidCollabMC (NO codebook)                   â”‚\n",
    "â”‚  â€¢ Purpose: Establish strong baseline                          â”‚\n",
    "â”‚  â€¢ Output: Pretrained weights for encoder/backbone/heads       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚                         â†“                                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  STAGE 2: Codebook-Only Training (20 epochs)                   â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â€¢ Model: HeterPyramidCollabCodebookMC (WITH codebook)         â”‚\n",
    "â”‚  â€¢ Load: Stage 1 checkpoint â†’ FREEZE all except codebook       â”‚\n",
    "â”‚  â€¢ Purpose: Learn optimal codebook for feature compression     â”‚\n",
    "â”‚  â€¢ Output: Trained codebook + frozen detector                  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚                         â†“                                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  STAGE 3: End-to-End Co-Training (10 epochs)                   â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â€¢ Model: HeterPyramidCollabCodebookMC                         â”‚\n",
    "â”‚  â€¢ Load: Stage 2 checkpoint â†’ UNFREEZE all parameters          â”‚\n",
    "â”‚  â€¢ Purpose: Joint optimization of codebook + detector          â”‚\n",
    "â”‚  â€¢ Output: Final model with optimal compression & accuracy     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚                         â†“                                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  STAGE 4: Post-Training Quantization (PTQ)                     â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â€¢ Quantize: Weights FP32â†’INT8, Activations FP32â†’INT8           â”‚\n",
    "â”‚  â€¢ Purpose: Inference speedup, Memory reduction                    â”‚\n",
    "â”‚  â€¢ Output: Fully quantized model ready for edge deployment     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                    QuantV2X: 3-Stage Training & PTQ Guide\n",
      "================================================================================\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L40S\n",
      "GPU Memory: 47.7 GB\n",
      "Device: cuda:0\n",
      "Project root: /home/zhihao/workspace/quantv2x_official\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure environment\n",
    "PROJECT_ROOT = os.path.abspath('../..')  # Navigate to quantv2x_official root\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "# Display system info\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 20 + \"QuantV2X: 3-Stage Training & PTQ Guide\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='roadmap'></a>\n",
    "## 2. Code Structure Roadmap\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "```\n",
    "quantv2x_official/\n",
    "â”œâ”€â”€ opencood/\n",
    "â”‚   â”œâ”€â”€ hypes_yaml/                    # Configuration files\n",
    "â”‚   â”‚   â””â”€â”€ v2x_real/Codebook/\n",
    "â”‚   â”‚       â”œâ”€â”€ stage1/\n",
    "â”‚   â”‚       â”‚   â””â”€â”€ lidar_pyramid_stage1.yaml      # Stage 1 config\n",
    "â”‚   â”‚       â”œâ”€â”€ stage2/\n",
    "â”‚   â”‚       â”‚   â””â”€â”€ lidar_pyramid_stage2.yaml      # Stage 2 config\n",
    "â”‚   â”‚       â””â”€â”€ stage3/\n",
    "â”‚   â”‚           â””â”€â”€ lidar_pyramid_stage3.yaml      # Stage 3 config\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ tools/                         # Training & inference scripts\n",
    "â”‚   â”‚   â”œâ”€â”€ train.py                   # Stage 1 training\n",
    "â”‚   â”‚   â”œâ”€â”€ train_ddp.py               # Stage 1 multi-GPU\n",
    "â”‚   â”‚   â”œâ”€â”€ train_stage2.py            # Stage 2 training\n",
    "â”‚   â”‚   â”œâ”€â”€ train_stage3.py            # Stage 3 training\n",
    "â”‚   â”‚   â”œâ”€â”€ inference_mc.py            # Full-precision inference\n",
    "â”‚   â”‚   â””â”€â”€ inference_mc_quant.py      # PTQ inference\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ models/                        # Model definitions\n",
    "â”‚   â”‚   â”œâ”€â”€ heter_pyramid_collab_mc.py              # Stage 1 model (no codebook)\n",
    "â”‚   â”‚   â”œâ”€â”€ heter_pyramid_collab_codebook_mc.py     # Stage 2/3 model (with codebook)\n",
    "â”‚   â”‚   â”œâ”€â”€ sub_modules/\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ codebook.py            # UMGMQuantizer implementation\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ pillar_vfe.py          # PointPillar encoder\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ base_bev_backbone_resnet.py   # ResNet backbone\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â”‚   â””â”€â”€ fuse_modules/\n",
    "â”‚   â”‚       â””â”€â”€ pyramid_fuse.py        # Pyramid fusion module\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ loss/                          # Loss functions\n",
    "â”‚   â”‚   â””â”€â”€ point_pillar_pyramid_loss_mc.py   # Multi-class loss\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ data_utils/                    # Data processing\n",
    "â”‚   â”‚   â””â”€â”€ datasets/\n",
    "â”‚   â”‚       â””â”€â”€ intermediate_fusion_dataset_mc_multistage.py\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ utils/                         # Utilities\n",
    "â”‚       â”œâ”€â”€ quant_utils.py             # PTQ utilities\n",
    "â”‚       â”œâ”€â”€ quant_model.py             # Quantized layer implementations\n",
    "â”‚       â””â”€â”€ ...\n",
    "â”‚\n",
    "â””â”€â”€ docs/\n",
    "    â”œâ”€â”€ Tutorial_V2X-Real_Baseline.md\n",
    "    â”œâ”€â”€ Tutorial_V2X-Real_Codebook.md\n",
    "    â””â”€â”€ notebook/\n",
    "        â””â”€â”€ QuantV2X_3Stage_PTQ_Guide.ipynb   # This notebook\n",
    "```\n",
    "\n",
    "### Key Files Overview\n",
    "\n",
    "#### 1. Configuration Files\n",
    "- **`lidar_pyramid_stage1.yaml`**: Full-precision baseline config\n",
    "  - Model: `heter_pyramid_collab_mc`\n",
    "  - No codebook parameters\n",
    "  - LR: 0.002, Batch: 8, Epochs: 20\n",
    "\n",
    "- **`lidar_pyramid_stage2.yaml`**: Codebook-only training config\n",
    "  - Model: `heter_pyramid_collab_codebook_mc`\n",
    "  - Codebook: seg_num=1, dict_size=128\n",
    "  - Freeze all except codebook\n",
    "  - LR: 0.002, Batch: 8, Epochs: 20\n",
    "\n",
    "- **`lidar_pyramid_stage3.yaml`**: End-to-end fine-tuning config\n",
    "  - Model: `heter_pyramid_collab_codebook_mc`\n",
    "  - Train all parameters\n",
    "  - LR: 0.0002 (10x smaller), Batch: 4, Epochs: 10\n",
    "\n",
    "#### 2. Training Scripts\n",
    "- **`train.py`**: Standard training loop for Stage 1\n",
    "- **`train_stage2.py`**: Loads Stage 1, freezes parameters, trains codebook\n",
    "- **`train_stage3.py`**: Loads Stage 2, unfreezes all, co-trains\n",
    "\n",
    "#### 3. Model Files\n",
    "- **`heter_pyramid_collab_mc.py`**: Baseline model without quantization\n",
    "- **`heter_pyramid_collab_codebook_mc.py`**: Model with codebook module\n",
    "- **`codebook.py`**: Core quantization module (UMGMQuantizer)\n",
    "\n",
    "#### 4. PTQ Files\n",
    "- **`inference_mc_quant.py`**: Inference with post-training quantization\n",
    "- **`quant_utils.py`**: Calibration and quantization utilities\n",
    "- **`quant_model.py`**: Quantized Conv2d, Linear, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='architecture'></a>\n",
    "## 3. Key Components & Architecture\n",
    "\n",
    "### Model Hierarchy\n",
    "\n",
    "```python\n",
    "# Stage 1 Model (No Codebook)\n",
    "HeterPyramidCollabMC(\n",
    "    encoder,           # PointPillar: Point cloud â†’ BEV features\n",
    "    backbone,          # ResNet: Feature extraction\n",
    "    aligner,           # Identity: Feature alignment\n",
    "    pyramid_backbone,  # PyramidFusion: Multi-scale fusion\n",
    "    shrink_header,     # Feature aggregation\n",
    "    cls_head,          # Classification head\n",
    "    reg_head,          # Regression head\n",
    "    dir_head           # Direction head\n",
    ")\n",
    "\n",
    "# Stage 2/3 Model (With Codebook)\n",
    "HeterPyramidCollabCodebookMC(\n",
    "    encoder,           # Same as above\n",
    "    backbone,          # Same as above\n",
    "    aligner,           # Same as above\n",
    "    codebook,          # â† NEW: UMGMQuantizer (codebook module)\n",
    "    pyramid_backbone,  # Same but operates on quantized features\n",
    "    shrink_header,     # Same as above\n",
    "    cls_head,          # Same as above\n",
    "    reg_head,          # Same as above\n",
    "    dir_head           # Same as above\n",
    ")\n",
    "```\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "Input: Multi-Agent Point Clouds [B, N_agents, N_points, 4]\n",
    "   â†“\n",
    "[Per-Agent Encoding]\n",
    "   PointPillar Encoder\n",
    "   â†’ BEV Features [B*N_agents, 64, H, W]\n",
    "   â†“\n",
    "[Per-Agent Backbone]\n",
    "   ResNet Backbone\n",
    "   â†’ Extracted Features [B*N_agents, 64, H/2, W/2]\n",
    "   â†“\n",
    "[Feature Alignment]\n",
    "   Aligner (Identity for LiDAR)\n",
    "   â†’ Aligned Features [B*N_agents, 64, H/2, W/2]\n",
    "   â†“\n",
    "[CODEBOOK QUANTIZATION] â† Stage 2/3 only!\n",
    "   Vector Quantization\n",
    "   â†’ Quantized Features [B*N_agents, 64, H/2, W/2]\n",
    "   â†’ Indices [B*N_agents, 1, H/2, W/2]  â† Transmitted!\n",
    "   â†“\n",
    "[Multi-Scale Pyramid Fusion]\n",
    "   Level 1 (1x): 64 channels\n",
    "   Level 2 (2x): 128 channels\n",
    "   Level 3 (4x): 256 channels\n",
    "   â†’ Fused Features [B, 384, H/2, W/2]\n",
    "   â†“\n",
    "[Feature Aggregation]\n",
    "   Shrink Header\n",
    "   â†’ Aggregated Features [B, 256, H/2, W/2]\n",
    "   â†“\n",
    "[Detection Heads]\n",
    "   Classification: [B, num_class*num_anchor*num_class, H/2, W/2]\n",
    "   Regression:     [B, 7*num_anchor*num_class, H/2, W/2]\n",
    "   Direction:      [B, 2*num_anchor*num_class, H/2, W/2]\n",
    "   â†“\n",
    "[Post-Processing]\n",
    "   Anchor decoding â†’ NMS â†’ Filtering\n",
    "   â†“\n",
    "Output: 3D Bounding Boxes [class, score, x, y, z, w, l, h, Î¸]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part-ii'></a>\n",
    "# Part II: 3-Stage Training Pipeline (Detailed)\n",
    "\n",
    "<a id='stage1'></a>\n",
    "## 4. Stage 1: Full-Precision Pretraining\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Stage 1 trains a **full-precision baseline model WITHOUT codebook quantization**. This serves as:\n",
    "1. **Performance upper bound**: Establishes best possible accuracy without compression\n",
    "2. **Stable initialization**: Provides pretrained weights for encoder, backbone, and detection heads\n",
    "3. **Architectural validation**: Ensures the base architecture works well\n",
    "\n",
    "### Configuration: `lidar_pyramid_stage1.yaml`\n",
    "\n",
    "Below is the **actual configuration** (not simplified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                    Stage 1 Configuration (Actual)\n",
      "================================================================================\n",
      "name: stage1_model\n",
      "root_dir: /data/dataset/v2xreal/train\n",
      "validate_dir: /data/dataset/v2xreal/test\n",
      "test_dir: /data/dataset/v2xreal/test\n",
      "yaml_parser: load_general_params\n",
      "train_params:\n",
      "  batch_size: 8\n",
      "  epoches: 20\n",
      "  eval_freq: 2\n",
      "  save_freq: 2\n",
      "  max_cav: 4\n",
      "comm_range: 70\n",
      "input_source:\n",
      "- lidar\n",
      "label_type: lidar\n",
      "cav_lidar_range: &id001\n",
      "- -140.8\n",
      "- -40\n",
      "- -3\n",
      "- 140.8\n",
      "- 40\n",
      "- 1\n",
      "num_class: 3\n",
      "dataset_mode: v2v\n",
      "heter:\n",
      "  assignment_path: opencood/modality_assign/v2xreal_4modality.json\n",
      "  ego_modality: m1\n",
      "  mapping_dict:\n",
      "    m1: none\n",
      "    m2: none\n",
      "    m3: m1\n",
      "    m4: m1\n",
      "  modality_setting:\n",
      "    m1:\n",
      "      sensor_type: lidar\n",
      "      core_method: point_pillar\n",
      "      preprocess:\n",
      "        core_method: SpVoxelPreprocessor\n",
      "        args:\n",
      "          voxel_size: &id002\n",
      "          - 0.4\n",
      "          - 0.4\n",
      "          - 4\n",
      "          max_points_per_voxel: 32\n",
      "          max_voxel_train: 32000\n",
      "          max_voxel_test: 70000\n",
      "        cav_lidar_range: *id001\n",
      "fusion:\n",
      "  core_method: intermediateheter3class\n",
      "  dataset: v2xreal\n",
      "  args:\n",
      "    proj_first: false\n",
      "    grid_conf: None\n",
      "    data_aug_conf: None\n",
      "preprocess:\n",
      "  core_method: SpVoxelPreprocessor\n",
      "  args:\n",
      "    voxel_size: *id002\n",
      "    max_points_per_voxel: 1\n",
      "    max_voxel_train: 1\n",
      "    max_voxel_test: 1\n",
      "  cav_lidar_range: *id001\n",
      "  num_class: 3\n",
      "  anchor_generator_config: &id003\n",
      "  - class_name: vehicle\n",
      "    anchor_sizes:\n",
      "    - - 3.9\n",
      "      - 1.6\n",
      "      - 1.56\n",
      "    anchor_rotations:\n",
      "    - 0\n",
      "    - 1.57\n",
      "    anchor_bottom_heights:\n",
      "    - -1.78\n",
      "    align_center: true\n",
      "    feature_map_stride: 2\n",
      "    matched_threshold: 0.6\n",
      "    unmatched_threshold: 0.45\n",
      "  - class_name: pedestrian\n",
      "    anchor_sizes:\n",
      "    - - 0.8\n",
      "      - 0.6\n",
      "      - 1.73\n",
      "    anchor_rotations:\n",
      "    - 0\n",
      "    - 1.57\n",
      "    anchor_bottom_heights:\n",
      "    - -0.6\n",
      "    align_center: true\n",
      "    feature_map_stride: 2\n",
      "    matched_threshold: 0.5\n",
      "    unmatched_threshold: 0.35\n",
      "  - class_name: truck\n",
      "    anchor_sizes:\n",
      "    - - 8\n",
      "      - 3\n",
      "      - 3\n",
      "    anchor_rotations:\n",
      "    - 0\n",
      "    - 1.57\n",
      "    anchor_bottom_heights:\n",
      "    - -1.78\n",
      "    align_center: true\n",
      "    feature_map_stride: 2\n",
      "    matched_threshold: 0.6\n",
      "    unmatched_threshold: 0.45\n",
      "postprocess:\n",
      "  core_method: VoxelPostprocessor3Heads\n",
      "  gt_range: *id001\n",
      "  anchor_args:\n",
      "    cav_lidar_range: *id001\n",
      "    l: 3.9\n",
      "    w: 1.6\n",
      "    h: 1.56\n",
      "    r: &id004\n",
      "    - 0\n",
      "    - 90\n",
      "    feature_stride: 2\n",
      "    num: 2\n",
      "    anchor_generator_config: *id003\n",
      "  target_args:\n",
      "    pos_threshold: 0.6\n",
      "    neg_threshold: 0.45\n",
      "    score_threshold: 0.2\n",
      "  order: hwl\n",
      "  max_num: 150\n",
      "  nms_thresh: 0.15\n",
      "  dir_args: &id005\n",
      "    dir_offset: 0.7853\n",
      "    num_bins: 2\n",
      "    anchor_yaw: *id004\n",
      "model:\n",
      "  core_method: heter_pyramid_collab_mc\n",
      "  args:\n",
      "    num_class: 3\n",
      "    lidar_range: *id001\n",
      "    supervise_single: true\n",
      "    m1:\n",
      "      core_method: point_pillar\n",
      "      sensor_type: lidar\n",
      "      encoder_args:\n",
      "        voxel_size: *id002\n",
      "        lidar_range: *id001\n",
      "        pillar_vfe:\n",
      "          use_norm: true\n",
      "          with_distance: false\n",
      "          use_absolute_xyz: true\n",
      "          num_filters:\n",
      "          - 64\n",
      "        point_pillar_scatter:\n",
      "          num_features: 64\n",
      "      backbone_args:\n",
      "        layer_nums:\n",
      "        - 3\n",
      "        layer_strides:\n",
      "        - 2\n",
      "        num_filters:\n",
      "        - 64\n",
      "      aligner_args:\n",
      "        core_method: identity\n",
      "    fusion_backbone:\n",
      "      resnext: true\n",
      "      stage: collab\n",
      "      layer_nums:\n",
      "      - 3\n",
      "      - 5\n",
      "      - 8\n",
      "      layer_strides:\n",
      "      - 1\n",
      "      - 2\n",
      "      - 2\n",
      "      num_filters:\n",
      "      - 64\n",
      "      - 128\n",
      "      - 256\n",
      "      upsample_strides:\n",
      "      - 1\n",
      "      - 2\n",
      "      - 4\n",
      "      num_upsample_filter:\n",
      "      - 128\n",
      "      - 128\n",
      "      - 128\n",
      "      anchor_number: 2\n",
      "    shrink_header:\n",
      "      kernal_size:\n",
      "      - 3\n",
      "      stride:\n",
      "      - 1\n",
      "      padding:\n",
      "      - 1\n",
      "      dim:\n",
      "      - 256\n",
      "      input_dim: 384\n",
      "    fusion_method: pyramid\n",
      "    in_head: 256\n",
      "    anchor_number: 2\n",
      "    dir_args: *id005\n",
      "loss:\n",
      "  core_method: point_pillar_pyramid_loss_mc\n",
      "  args:\n",
      "    num_class: 3\n",
      "    pos_cls_weight: 2.0\n",
      "    cls:\n",
      "      type: SigmoidFocalLoss\n",
      "      alpha: 0.25\n",
      "      gamma: 2.0\n",
      "      weight: 1.0\n",
      "    reg:\n",
      "      type: WeightedSmoothL1Loss\n",
      "      sigma: 3.0\n",
      "      codewise: true\n",
      "      weight: 2.0\n",
      "    dir:\n",
      "      type: WeightedSoftmaxClassificationLoss\n",
      "      weight: 0.2\n",
      "      args: *id005\n",
      "    depth:\n",
      "      weight: 1.0\n",
      "    pyramid:\n",
      "      relative_downsample:\n",
      "      - 1\n",
      "      - 2\n",
      "      - 4\n",
      "      weight:\n",
      "      - 0.4\n",
      "      - 0.2\n",
      "      - 0.1\n",
      "optimizer:\n",
      "  core_method: Adam\n",
      "  lr: 0.002\n",
      "  args:\n",
      "    eps: 1e-10\n",
      "    weight_decay: 1e-4\n",
      "lr_scheduler:\n",
      "  core_method: multistep\n",
      "  gamma: 0.1\n",
      "  step_size:\n",
      "  - 15\n",
      "  - 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and display the actual Stage 1 configuration\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "stage1_config_path = Path(PROJECT_ROOT) / \"opencood\" / \"hypes_yaml\" / \"v2x_real\" / \"Codebook\" / \"stage1\" / \"lidar_pyramid_stage1.yaml\"\n",
    "\n",
    "if stage1_config_path.exists():\n",
    "    with open(stage1_config_path, 'r') as f:\n",
    "        stage1_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\" \" * 20 + \"Stage 1 Configuration (Actual)\")\n",
    "    print(\"=\"*80)\n",
    "    print(yaml.dump(stage1_config, default_flow_style=False, sort_keys=False))\n",
    "else:\n",
    "    print(f\"Configuration file not found: {stage1_config_path}\")\n",
    "    print(\"\\nTypical Stage 1 configuration:\")\n",
    "    print(\"\"\"\n",
    "name: lidar_pyramid_stage1\n",
    "data_dir: \"/data/dataset/v2xreal\"\n",
    "root_dir: \"opencood/logs/stage1_model\"\n",
    "\n",
    "train_params:\n",
    "  batch_size: 8\n",
    "  epoches: 20\n",
    "  eval_freq: 2\n",
    "  save_freq: 2\n",
    "  max_cav: 4\n",
    "\n",
    "model:\n",
    "  core_method: heter_pyramid_collab_mc  # NO codebook!\n",
    "  args:\n",
    "    num_class: 3\n",
    "    lidar_range: [-140.8, -40, -3, 140.8, 40, 1]\n",
    "    supervise_single: True\n",
    "    fusion_method: pyramid\n",
    "    \n",
    "optimizer:\n",
    "  core_method: Adam\n",
    "  lr: 0.002\n",
    "  \n",
    "lr_scheduler:\n",
    "  core_method: multistep\n",
    "  gamma: 0.1\n",
    "  step_size: [15, 25]\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script: `train.py`\n",
    "\n",
    "Let's examine the **actual training loop** from `opencood/tools/train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "# opencood/tools/train.py (Key sections)\n",
       "\n",
       "import torch\n",
       "from torch.utils.data import DataLoader\n",
       "from opencood.hypes_yaml.yaml_utils import load_yaml\n",
       "from opencood.data_utils.datasets import build_dataset\n",
       "\n",
       "def train_parser():\n",
       "    parser = argparse.ArgumentParser(description=\"QuantV2X training\")\n",
       "    parser.add_argument(\"-y\", \"--hypes_yaml\", required=True,\n",
       "                        help=\"Path to config YAML file\")\n",
       "    parser.add_argument(\"--model_dir\", default=\"\",\n",
       "                        help=\"Path to resume from checkpoint\")\n",
       "    args = parser.parse_args()\n",
       "    return args\n",
       "\n",
       "def main():\n",
       "    args = train_parser()\n",
       "    hypes = load_yaml(args.hypes_yaml)\n",
       "    \n",
       "    # ============================================================\n",
       "    # 1. Create dataset\n",
       "    # ============================================================\n",
       "    print(\"Creating dataset...\")\n",
       "    opencood_train_dataset = build_dataset(\n",
       "        hypes, \n",
       "        visualize=False, \n",
       "        train=True\n",
       "    )\n",
       "    opencood_validate_dataset = build_dataset(\n",
       "        hypes,\n",
       "        visualize=False,\n",
       "        train=False\n",
       "    )\n",
       "    \n",
       "    train_loader = DataLoader(\n",
       "        opencood_train_dataset,\n",
       "        batch_size=hypes['train_params']['batch_size'],\n",
       "        num_workers=8,\n",
       "        shuffle=True,\n",
       "        pin_memory=True,\n",
       "        drop_last=True\n",
       "    )\n",
       "    \n",
       "    val_loader = DataLoader(\n",
       "        opencood_validate_dataset,\n",
       "        batch_size=hypes['train_params']['batch_size'],\n",
       "        num_workers=8,\n",
       "        shuffle=False,\n",
       "        pin_memory=True,\n",
       "        drop_last=True\n",
       "    )\n",
       "    \n",
       "    # ============================================================\n",
       "    # 2. Build model\n",
       "    # ============================================================\n",
       "    print(\"Building model...\")\n",
       "    model = train_utils.create_model(hypes)  # Creates HeterPyramidCollabMC\n",
       "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "    model.to(device)\n",
       "    \n",
       "    # ============================================================\n",
       "    # 3. Define optimizer and scheduler\n",
       "    # ============================================================\n",
       "    optimizer = train_utils.setup_optimizer(\n",
       "        hypes, \n",
       "        model\n",
       "    )  # Adam with lr=0.002\n",
       "    \n",
       "    scheduler = train_utils.setup_lr_scheduler(\n",
       "        hypes,\n",
       "        optimizer\n",
       "    )  # MultiStepLR with milestones=[15, 25]\n",
       "    \n",
       "    # ============================================================\n",
       "    # 4. Define loss function\n",
       "    # ============================================================\n",
       "    criterion = train_utils.create_loss(hypes)  # PointPillarPyramidLossMC\n",
       "    \n",
       "    # ============================================================\n",
       "    # 5. Training loop\n",
       "    # ============================================================\n",
       "    epoches = hypes['train_params']['epoches']\n",
       "    save_freq = hypes['train_params']['save_freq']\n",
       "    eval_freq = hypes['train_params']['eval_freq']\n",
       "    \n",
       "    for epoch in range(epoches):\n",
       "        # Training phase\n",
       "        model.train()\n",
       "        for i, batch_data in enumerate(train_loader):\n",
       "            # Move data to GPU\n",
       "            batch_data = train_utils.to_device(batch_data, device)\n",
       "            \n",
       "            # Forward pass\n",
       "            output_dict = model(batch_data['ego'])\n",
       "            \n",
       "            # Compute loss\n",
       "            final_loss = criterion(\n",
       "                output_dict,\n",
       "                batch_data['ego']['label_dict']\n",
       "            )\n",
       "            \n",
       "            # Backward pass\n",
       "            optimizer.zero_grad()\n",
       "            final_loss.backward()\n",
       "            optimizer.step()\n",
       "            \n",
       "            # Log\n",
       "            if i % 10 == 0:\n",
       "                print(f\"Epoch {epoch}, Iter {i}, Loss: {final_loss.item():.4f}\")\n",
       "        \n",
       "        # Update learning rate\n",
       "        scheduler.step()\n",
       "        \n",
       "        # Validation\n",
       "        if (epoch + 1) % eval_freq == 0:\n",
       "            model.eval()\n",
       "            with torch.no_grad():\n",
       "                val_loss = 0.0\n",
       "                for batch_data in val_loader:\n",
       "                    batch_data = train_utils.to_device(batch_data, device)\n",
       "                    output_dict = model(batch_data['ego'])\n",
       "                    loss = criterion(output_dict, batch_data['ego']['label_dict'])\n",
       "                    val_loss += loss.item()\n",
       "                val_loss /= len(val_loader)\n",
       "                print(f\"Epoch {epoch}, Validation Loss: {val_loss:.4f}\")\n",
       "        \n",
       "        # Save checkpoint\n",
       "        if (epoch + 1) % save_freq == 0:\n",
       "            torch.save(\n",
       "                model.state_dict(),\n",
       "                f\"opencood/logs/stage1_model/net_epoch_{epoch+1}.pth\"\n",
       "            )\n",
       "    \n",
       "    print(\"Training completed!\")\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    main()\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display relevant parts of the training script\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "train_code = '''\n",
    "```python\n",
    "# opencood/tools/train.py (Key sections)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from opencood.hypes_yaml.yaml_utils import load_yaml\n",
    "from opencood.data_utils.datasets import build_dataset\n",
    "\n",
    "def train_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"QuantV2X training\")\n",
    "    parser.add_argument(\"-y\", \"--hypes_yaml\", required=True,\n",
    "                        help=\"Path to config YAML file\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"\",\n",
    "                        help=\"Path to resume from checkpoint\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = train_parser()\n",
    "    hypes = load_yaml(args.hypes_yaml)\n",
    "    \n",
    "    # ============================================================\n",
    "    # 1. Create dataset\n",
    "    # ============================================================\n",
    "    print(\"Creating dataset...\")\n",
    "    opencood_train_dataset = build_dataset(\n",
    "        hypes, \n",
    "        visualize=False, \n",
    "        train=True\n",
    "    )\n",
    "    opencood_validate_dataset = build_dataset(\n",
    "        hypes,\n",
    "        visualize=False,\n",
    "        train=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        opencood_train_dataset,\n",
    "        batch_size=hypes['train_params']['batch_size'],\n",
    "        num_workers=8,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        opencood_validate_dataset,\n",
    "        batch_size=hypes['train_params']['batch_size'],\n",
    "        num_workers=8,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. Build model\n",
    "    # ============================================================\n",
    "    print(\"Building model...\")\n",
    "    model = train_utils.create_model(hypes)  # Creates HeterPyramidCollabMC\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. Define optimizer and scheduler\n",
    "    # ============================================================\n",
    "    optimizer = train_utils.setup_optimizer(\n",
    "        hypes, \n",
    "        model\n",
    "    )  # Adam with lr=0.002\n",
    "    \n",
    "    scheduler = train_utils.setup_lr_scheduler(\n",
    "        hypes,\n",
    "        optimizer\n",
    "    )  # MultiStepLR with milestones=[15, 25]\n",
    "    \n",
    "    # ============================================================\n",
    "    # 4. Define loss function\n",
    "    # ============================================================\n",
    "    criterion = train_utils.create_loss(hypes)  # PointPillarPyramidLossMC\n",
    "    \n",
    "    # ============================================================\n",
    "    # 5. Training loop\n",
    "    # ============================================================\n",
    "    epoches = hypes['train_params']['epoches']\n",
    "    save_freq = hypes['train_params']['save_freq']\n",
    "    eval_freq = hypes['train_params']['eval_freq']\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            # Move data to GPU\n",
    "            batch_data = train_utils.to_device(batch_data, device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output_dict = model(batch_data['ego'])\n",
    "            \n",
    "            # Compute loss\n",
    "            final_loss = criterion(\n",
    "                output_dict,\n",
    "                batch_data['ego']['label_dict']\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Iter {i}, Loss: {final_loss.item():.4f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % eval_freq == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                for batch_data in val_loader:\n",
    "                    batch_data = train_utils.to_device(batch_data, device)\n",
    "                    output_dict = model(batch_data['ego'])\n",
    "                    loss = criterion(output_dict, batch_data['ego']['label_dict'])\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= len(val_loader)\n",
    "                print(f\"Epoch {epoch}, Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"opencood/logs/stage1_model/net_epoch_{epoch+1}.pth\"\n",
    "            )\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(train_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture: `HeterPyramidCollabMC`\n",
    "\n",
    "Let's examine the **actual model implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "# opencood/models/heter_pyramid_collab_mc.py (Simplified but not oversimplified)\n",
       "\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "from opencood.models.sub_modules.pillar_vfe import PillarVFE\n",
       "from opencood.models.sub_modules.point_pillar_scatter import PointPillarScatter\n",
       "from opencood.models.sub_modules.base_bev_backbone_resnet import ResNetBEVBackbone\n",
       "from opencood.models.sub_modules.downsample_conv import DownsampleConv\n",
       "from opencood.models.sub_modules.naive_compress import NaiveCompressor\n",
       "from opencood.models.fuse_modules.pyramid_fuse import PyramidFusion\n",
       "\n",
       "class HeterPyramidCollabMC(nn.Module):\n",
       "    \"\"\"\n",
       "    Heterogeneous Pyramid Collaborative Perception for Multi-Class Detection\n",
       "    (Stage 1: Full-precision baseline WITHOUT codebook)\n",
       "    \"\"\"\n",
       "    \n",
       "    def __init__(self, args):\n",
       "        super(HeterPyramidCollabMC, self).__init__()\n",
       "        \n",
       "        # ========================================\n",
       "        # 1. Per-Modality Encoders\n",
       "        # ========================================\n",
       "        # For modality 1 (LiDAR)\n",
       "        self.pillar_vfe_m1 = PillarVFE(\n",
       "            args['pillar_vfe'],\n",
       "            num_point_features=4,\n",
       "            voxel_size=args['voxel_size'],\n",
       "            point_cloud_range=args['lidar_range']\n",
       "        )\n",
       "        self.scatter_m1 = PointPillarScatter(args['point_pillar_scatter'])\n",
       "        \n",
       "        # ========================================\n",
       "        # 2. Per-Modality Backbones\n",
       "        # ========================================\n",
       "        self.backbone_m1 = ResNetBEVBackbone(\n",
       "            args['base_bev_backbone'],\n",
       "            64  # Input channels from scatter\n",
       "        )\n",
       "        \n",
       "        # ========================================\n",
       "        # 3. Feature Aligners\n",
       "        # ========================================\n",
       "        # For LiDAR-only, use identity (no alignment needed)\n",
       "        self.aligner_m1 = nn.Identity()\n",
       "        \n",
       "        # ========================================\n",
       "        # 4. Pyramid Fusion Backbone\n",
       "        # ========================================\n",
       "        self.fusion_net = PyramidFusion(args['fusion_args'])\n",
       "        \n",
       "        # ========================================\n",
       "        # 5. Shrink Header (Feature Aggregation)\n",
       "        # ========================================\n",
       "        self.shrink_flag = False\n",
       "        if 'shrink_header' in args:\n",
       "            self.shrink_flag = True\n",
       "            self.shrink_conv = DownsampleConv(args['shrink_header'])\n",
       "        \n",
       "        # ========================================\n",
       "        # 6. Detection Heads\n",
       "        # ========================================\n",
       "        self.num_class = args['num_class']  # 3: vehicle, pedestrian, truck\n",
       "        self.anchor_num = args['anchor_num']  # 2 rotations per class\n",
       "        \n",
       "        # Classification head\n",
       "        self.cls_head = nn.Conv2d(\n",
       "            256,  # Input channels after shrink\n",
       "            self.anchor_num * self.num_class,\n",
       "            kernel_size=1\n",
       "        )\n",
       "        \n",
       "        # Regression head (7 DOF: x, y, z, w, l, h, theta)\n",
       "        self.reg_head = nn.Conv2d(\n",
       "            256,\n",
       "            7 * self.anchor_num * self.num_class,\n",
       "            kernel_size=1\n",
       "        )\n",
       "        \n",
       "        # Direction head (2 bins for heading)\n",
       "        if 'dir_args' in args:\n",
       "            self.use_dir = True\n",
       "            self.dir_head = nn.Conv2d(\n",
       "                256,\n",
       "                2 * self.anchor_num * self.num_class,\n",
       "                kernel_size=1\n",
       "            )\n",
       "        else:\n",
       "            self.use_dir = False\n",
       "    \n",
       "    def forward(self, data_dict):\n",
       "        \"\"\"\n",
       "        Forward pass\n",
       "        \n",
       "        Args:\n",
       "            data_dict: Dictionary containing:\n",
       "                - 'processed_lidar': dict with voxel features\n",
       "                - 'record_len': number of agents per scene\n",
       "                - 'pairwise_t_matrix': transformation matrices\n",
       "        \n",
       "        Returns:\n",
       "            output_dict: Dictionary containing:\n",
       "                - 'cls_preds': classification predictions\n",
       "                - 'reg_preds': regression predictions\n",
       "                - 'dir_preds': direction predictions (optional)\n",
       "        \"\"\"\n",
       "        \n",
       "        # ========================================\n",
       "        # 1. Per-Agent Encoding (PointPillar)\n",
       "        # ========================================\n",
       "        voxel_features = data_dict['processed_lidar']['voxel_features']\n",
       "        voxel_coords = data_dict['processed_lidar']['voxel_coords']\n",
       "        voxel_num_points = data_dict['processed_lidar']['voxel_num_points']\n",
       "        \n",
       "        # Pillar VFE: [N_pillars, 32, 9] â†’ [N_pillars, 64]\n",
       "        batch_dict = {\n",
       "            'voxel_features': voxel_features,\n",
       "            'voxel_coords': voxel_coords,\n",
       "            'voxel_num_points': voxel_num_points\n",
       "        }\n",
       "        batch_dict = self.pillar_vfe_m1(batch_dict)\n",
       "        \n",
       "        # Scatter to BEV: [N_pillars, 64] â†’ [B, 64, H, W]\n",
       "        batch_dict = self.scatter_m1(batch_dict)\n",
       "        spatial_features = batch_dict['spatial_features']\n",
       "        \n",
       "        # ========================================\n",
       "        # 2. Per-Agent Backbone (ResNet)\n",
       "        # ========================================\n",
       "        spatial_features_2d = self.backbone_m1(spatial_features)['spatial_features_2d']\n",
       "        \n",
       "        # ========================================\n",
       "        # 3. Feature Alignment\n",
       "        # ========================================\n",
       "        psm_single = self.aligner_m1(spatial_features_2d)\n",
       "        \n",
       "        # ========================================\n",
       "        # 4. Multi-Scale Pyramid Fusion\n",
       "        # ========================================\n",
       "        # Note: In Stage 1, NO codebook quantization happens here!\n",
       "        record_len = data_dict['record_len']\n",
       "        pairwise_t_matrix = data_dict['pairwise_t_matrix']\n",
       "        \n",
       "        fused_feature = self.fusion_net(\n",
       "            psm_single,\n",
       "            record_len,\n",
       "            pairwise_t_matrix\n",
       "        )\n",
       "        \n",
       "        # ========================================\n",
       "        # 5. Shrink Header\n",
       "        # ========================================\n",
       "        if self.shrink_flag:\n",
       "            fused_feature = self.shrink_conv(fused_feature)\n",
       "        \n",
       "        # ========================================\n",
       "        # 6. Detection Heads\n",
       "        # ========================================\n",
       "        cls_preds = self.cls_head(fused_feature)\n",
       "        reg_preds = self.reg_head(fused_feature)\n",
       "        \n",
       "        output_dict = {\n",
       "            'cls_preds': cls_preds,\n",
       "            'reg_preds': reg_preds,\n",
       "            'psm': fused_feature\n",
       "        }\n",
       "        \n",
       "        if self.use_dir:\n",
       "            dir_preds = self.dir_head(fused_feature)\n",
       "            output_dict['dir_preds'] = dir_preds\n",
       "        \n",
       "        return output_dict\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display model architecture code\n",
    "model_code = '''\n",
    "```python\n",
    "# opencood/models/heter_pyramid_collab_mc.py (Simplified but not oversimplified)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from opencood.models.sub_modules.pillar_vfe import PillarVFE\n",
    "from opencood.models.sub_modules.point_pillar_scatter import PointPillarScatter\n",
    "from opencood.models.sub_modules.base_bev_backbone_resnet import ResNetBEVBackbone\n",
    "from opencood.models.sub_modules.downsample_conv import DownsampleConv\n",
    "from opencood.models.sub_modules.naive_compress import NaiveCompressor\n",
    "from opencood.models.fuse_modules.pyramid_fuse import PyramidFusion\n",
    "\n",
    "class HeterPyramidCollabMC(nn.Module):\n",
    "    \"\"\"\n",
    "    Heterogeneous Pyramid Collaborative Perception for Multi-Class Detection\n",
    "    (Stage 1: Full-precision baseline WITHOUT codebook)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(HeterPyramidCollabMC, self).__init__()\n",
    "        \n",
    "        # ========================================\n",
    "        # 1. Per-Modality Encoders\n",
    "        # ========================================\n",
    "        # For modality 1 (LiDAR)\n",
    "        self.pillar_vfe_m1 = PillarVFE(\n",
    "            args['pillar_vfe'],\n",
    "            num_point_features=4,\n",
    "            voxel_size=args['voxel_size'],\n",
    "            point_cloud_range=args['lidar_range']\n",
    "        )\n",
    "        self.scatter_m1 = PointPillarScatter(args['point_pillar_scatter'])\n",
    "        \n",
    "        # ========================================\n",
    "        # 2. Per-Modality Backbones\n",
    "        # ========================================\n",
    "        self.backbone_m1 = ResNetBEVBackbone(\n",
    "            args['base_bev_backbone'],\n",
    "            64  # Input channels from scatter\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # 3. Feature Aligners\n",
    "        # ========================================\n",
    "        # For LiDAR-only, use identity (no alignment needed)\n",
    "        self.aligner_m1 = nn.Identity()\n",
    "        \n",
    "        # ========================================\n",
    "        # 4. Pyramid Fusion Backbone\n",
    "        # ========================================\n",
    "        self.fusion_net = PyramidFusion(args['fusion_args'])\n",
    "        \n",
    "        # ========================================\n",
    "        # 5. Shrink Header (Feature Aggregation)\n",
    "        # ========================================\n",
    "        self.shrink_flag = False\n",
    "        if 'shrink_header' in args:\n",
    "            self.shrink_flag = True\n",
    "            self.shrink_conv = DownsampleConv(args['shrink_header'])\n",
    "        \n",
    "        # ========================================\n",
    "        # 6. Detection Heads\n",
    "        # ========================================\n",
    "        self.num_class = args['num_class']  # 3: vehicle, pedestrian, truck\n",
    "        self.anchor_num = args['anchor_num']  # 2 rotations per class\n",
    "        \n",
    "        # Classification head\n",
    "        self.cls_head = nn.Conv2d(\n",
    "            256,  # Input channels after shrink\n",
    "            self.anchor_num * self.num_class,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "        # Regression head (7 DOF: x, y, z, w, l, h, theta)\n",
    "        self.reg_head = nn.Conv2d(\n",
    "            256,\n",
    "            7 * self.anchor_num * self.num_class,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "        # Direction head (2 bins for heading)\n",
    "        if 'dir_args' in args:\n",
    "            self.use_dir = True\n",
    "            self.dir_head = nn.Conv2d(\n",
    "                256,\n",
    "                2 * self.anchor_num * self.num_class,\n",
    "                kernel_size=1\n",
    "            )\n",
    "        else:\n",
    "            self.use_dir = False\n",
    "    \n",
    "    def forward(self, data_dict):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            data_dict: Dictionary containing:\n",
    "                - 'processed_lidar': dict with voxel features\n",
    "                - 'record_len': number of agents per scene\n",
    "                - 'pairwise_t_matrix': transformation matrices\n",
    "        \n",
    "        Returns:\n",
    "            output_dict: Dictionary containing:\n",
    "                - 'cls_preds': classification predictions\n",
    "                - 'reg_preds': regression predictions\n",
    "                - 'dir_preds': direction predictions (optional)\n",
    "        \"\"\"\n",
    "        \n",
    "        # ========================================\n",
    "        # 1. Per-Agent Encoding (PointPillar)\n",
    "        # ========================================\n",
    "        voxel_features = data_dict['processed_lidar']['voxel_features']\n",
    "        voxel_coords = data_dict['processed_lidar']['voxel_coords']\n",
    "        voxel_num_points = data_dict['processed_lidar']['voxel_num_points']\n",
    "        \n",
    "        # Pillar VFE: [N_pillars, 32, 9] â†’ [N_pillars, 64]\n",
    "        batch_dict = {\n",
    "            'voxel_features': voxel_features,\n",
    "            'voxel_coords': voxel_coords,\n",
    "            'voxel_num_points': voxel_num_points\n",
    "        }\n",
    "        batch_dict = self.pillar_vfe_m1(batch_dict)\n",
    "        \n",
    "        # Scatter to BEV: [N_pillars, 64] â†’ [B, 64, H, W]\n",
    "        batch_dict = self.scatter_m1(batch_dict)\n",
    "        spatial_features = batch_dict['spatial_features']\n",
    "        \n",
    "        # ========================================\n",
    "        # 2. Per-Agent Backbone (ResNet)\n",
    "        # ========================================\n",
    "        spatial_features_2d = self.backbone_m1(spatial_features)['spatial_features_2d']\n",
    "        \n",
    "        # ========================================\n",
    "        # 3. Feature Alignment\n",
    "        # ========================================\n",
    "        psm_single = self.aligner_m1(spatial_features_2d)\n",
    "        \n",
    "        # ========================================\n",
    "        # 4. Multi-Scale Pyramid Fusion\n",
    "        # ========================================\n",
    "        # Note: In Stage 1, NO codebook quantization happens here!\n",
    "        record_len = data_dict['record_len']\n",
    "        pairwise_t_matrix = data_dict['pairwise_t_matrix']\n",
    "        \n",
    "        fused_feature = self.fusion_net(\n",
    "            psm_single,\n",
    "            record_len,\n",
    "            pairwise_t_matrix\n",
    "        )\n",
    "        \n",
    "        # ========================================\n",
    "        # 5. Shrink Header\n",
    "        # ========================================\n",
    "        if self.shrink_flag:\n",
    "            fused_feature = self.shrink_conv(fused_feature)\n",
    "        \n",
    "        # ========================================\n",
    "        # 6. Detection Heads\n",
    "        # ========================================\n",
    "        cls_preds = self.cls_head(fused_feature)\n",
    "        reg_preds = self.reg_head(fused_feature)\n",
    "        \n",
    "        output_dict = {\n",
    "            'cls_preds': cls_preds,\n",
    "            'reg_preds': reg_preds,\n",
    "            'psm': fused_feature\n",
    "        }\n",
    "        \n",
    "        if self.use_dir:\n",
    "            dir_preds = self.dir_head(fused_feature)\n",
    "            output_dict['dir_preds'] = dir_preds\n",
    "        \n",
    "        return output_dict\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(model_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Training Commands\n",
    "\n",
    "```bash\n",
    "# Single GPU training\n",
    "python ./opencood/tools/train.py \\\n",
    "    -y ./opencood/hypes_yaml/v2x_real/Codebook/stage1/lidar_pyramid_stage1.yaml\n",
    "\n",
    "# Multi-GPU training (recommended)\n",
    "CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch \\\n",
    "    --nproc_per_node=2 --use_env \\\n",
    "    ./opencood/tools/train_ddp.py \\\n",
    "    -y ./opencood/hypes_yaml/v2x_real/Codebook/stage1/lidar_pyramid_stage1.yaml\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "```\n",
    "opencood/logs/\n",
    "â””â”€â”€ stage1_model_YYYY_MM_DD_HH_MM_SS/\n",
    "    â”œâ”€â”€ config.yaml\n",
    "    â”œâ”€â”€ net_epoch_1.pth\n",
    "    â”œâ”€â”€ net_epoch_2.pth\n",
    "    â”œâ”€â”€ ...\n",
    "    â”œâ”€â”€ net_epoch_20.pth\n",
    "    â””â”€â”€ net_epoch_best.pth       # â† Use this for Stage 2!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='stage2'></a>\n",
    "## 5. Stage 2: Codebook-Only Training\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Stage 2 focuses on **learning the codebook quantization** while keeping the rest of the model frozen:\n",
    "1. **Load pretrained weights** from Stage 1\n",
    "2. **Add codebook module** (randomly initialized)\n",
    "3. **Freeze encoder, backbone, and detection heads**\n",
    "4. **Train only codebook parameters**\n",
    "\n",
    "This prevents catastrophic forgetting of the pretrained detector while learning optimal feature compression.\n",
    "\n",
    "### Key Differences from Stage 1\n",
    "\n",
    "| Aspect | Stage 1 | Stage 2 |\n",
    "|--------|---------|----------|\n",
    "| Model | `heter_pyramid_collab_mc` | `heter_pyramid_collab_codebook_mc` |\n",
    "| Codebook | âŒ None | âœ… seg_num=1, dict_size=128 |\n",
    "| Training | All parameters | **Codebook only** |\n",
    "| Initialization | Random | **Load Stage 1 checkpoint** |\n",
    "| Batch Size | 8 | 8 (same) |\n",
    "| Learning Rate | 0.002 | 0.002 (same) |\n",
    "| Epochs | 20 | 20 (same) |\n",
    "\n",
    "### Configuration Diff\n",
    "\n",
    "The main difference in the YAML configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                         Stage 1 vs Stage 2 Config Diff\n",
      "================================================================================\n",
      "\n",
      "[STAGE 1]\n",
      "model:\n",
      "  core_method: heter_pyramid_collab_mc  # NO codebook\n",
      "  args:\n",
      "    num_class: 3\n",
      "    lidar_range: [-140.8, -40, -3, 140.8, 40, 1]\n",
      "    # No codebook configuration\n",
      "\n",
      "[STAGE 2]\n",
      "model:\n",
      "  core_method: heter_pyramid_collab_codebook_mc  # WITH codebook\n",
      "  args:\n",
      "    num_class: 3\n",
      "    lidar_range: [-140.8, -40, -3, 140.8, 40, 1]\n",
      "    # NEW: Codebook configuration\n",
      "    codebook:\n",
      "      seg_num: 1          # Number of segments (groups)\n",
      "      dict_size: 128      # Codebook size per segment\n",
      "    use_codebook: true\n",
      "================================================================================\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Show configuration differences\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 25 + \"Stage 1 vs Stage 2 Config Diff\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[STAGE 1]\")\n",
    "print(\"model:\")\n",
    "print(\"  core_method: heter_pyramid_collab_mc  # NO codebook\")\n",
    "print(\"  args:\")\n",
    "print(\"    num_class: 3\")\n",
    "print(\"    lidar_range: [-140.8, -40, -3, 140.8, 40, 1]\")\n",
    "print(\"    # No codebook configuration\")\n",
    "\n",
    "print(\"\\n[STAGE 2]\")\n",
    "print(\"model:\")\n",
    "print(\"  core_method: heter_pyramid_collab_codebook_mc  # WITH codebook\")\n",
    "print(\"  args:\")\n",
    "print(\"    num_class: 3\")\n",
    "print(\"    lidar_range: [-140.8, -40, -3, 140.8, 40, 1]\")\n",
    "print(\"    # NEW: Codebook configuration\")\n",
    "print(\"    codebook:\")\n",
    "print(\"      seg_num: 1          # Number of segments (groups)\")\n",
    "print(\"      dict_size: 128      # Codebook size per segment\")\n",
    "print(\"    use_codebook: true\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebook Implementation: `UMGMQuantizer`\n",
    "\n",
    "The core quantization module is implemented in `opencood/models/sub_modules/codebook.py`.\n",
    "\n",
    "Below is the **actual implementation** (key parts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "# opencood/models/sub_modules/codebook.py (Core quantization logic)\n",
       "\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import math\n",
       "\n",
       "class _multiCodebookQuantization(nn.Module):\n",
       "    \"\"\"\n",
       "    Multi-codebook vector quantization module.\n",
       "    \n",
       "    Args:\n",
       "        codebook: [m, k, d] learnable codebook\n",
       "            m: number of segments\n",
       "            k: dict_size (number of codewords per segment)\n",
       "            d: channel dimension per segment (total_channels // m)\n",
       "        permutationRate: probability of random perturbation\n",
       "    \"\"\"\n",
       "    \n",
       "    def __init__(self, codebook: nn.Parameter, permutationRate: float = 0.0):\n",
       "        super().__init__()\n",
       "        self._m, self._k, self._d = codebook.shape  # e.g., [1, 128, 64]\n",
       "        self._codebook = codebook  # Learnable!\n",
       "        self._scale = math.sqrt(self._k)\n",
       "        self._temperature = nn.Parameter(torch.ones((self._m, 1)))\n",
       "    \n",
       "    def _distance(self, x: torch.Tensor) -> torch.Tensor:\n",
       "        \"\"\"\n",
       "        Compute L2 distance between input features and all codewords.\n",
       "        \n",
       "        Args:\n",
       "            x: [n, c] input features (c = m * d)\n",
       "        \n",
       "        Returns:\n",
       "            distance: [n, m, k] distances to each codeword\n",
       "        \"\"\"\n",
       "        n, _ = x.shape\n",
       "        # Reshape: [n, c] â†’ [n, m, d]\n",
       "        x = x.reshape(n, self._m, self._d)\n",
       "        \n",
       "        # Squared norm of input: [n, m, 1]\n",
       "        x2 = (x ** 2).sum(2, keepdim=True)\n",
       "        \n",
       "        # Squared norm of codebook: [m, k]\n",
       "        c2 = (self._codebook ** 2).sum(-1, keepdim=False)\n",
       "        \n",
       "        # Inner product: [n, m, k]\n",
       "        inter = torch.einsum(\"nmd,mkd->nmk\", x, self._codebook)\n",
       "        \n",
       "        # L2 distance: ||x - c||^2 = ||x||^2 + ||c||^2 - 2<x, c>\n",
       "        distance = x2 + c2 - 2 * inter\n",
       "        \n",
       "        return distance\n",
       "    \n",
       "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
       "        \"\"\"\n",
       "        Encode input features to codebook indices.\n",
       "        \n",
       "        Args:\n",
       "            x: [n, c] input features\n",
       "        \n",
       "        Returns:\n",
       "            code: [n, m] codebook indices\n",
       "        \"\"\"\n",
       "        distance = self._distance(x)  # [n, m, k]\n",
       "        code = distance.argmin(-1)     # [n, m] - nearest codeword\n",
       "        return code\n",
       "    \n",
       "    def forward(self, x: torch.Tensor):\n",
       "        \"\"\"\n",
       "        Forward pass with Gumbel-Softmax for differentiable sampling.\n",
       "        \n",
       "        Args:\n",
       "            x: [n, c] input features\n",
       "        \n",
       "        Returns:\n",
       "            sample: [n, m, k] soft assignment (differentiable)\n",
       "            code: [n, m] hard assignment (indices)\n",
       "            oneHot: [n, m, k] one-hot encoding\n",
       "            logit: [n, m, k] logits for probability distribution\n",
       "        \"\"\"\n",
       "        # Compute logits (negative distance)\n",
       "        logit = -1 * self._distance(x) / self._scale\n",
       "        \n",
       "        # Gumbel-Softmax sampling (differentiable!)\n",
       "        sample = gumbelSoftmax(logit, temperature=1.0, hard=True)\n",
       "        \n",
       "        # Get hard assignment\n",
       "        code = logit.argmax(-1, keepdim=True)  # [n, m, 1]\n",
       "        \n",
       "        # One-hot encoding\n",
       "        oneHot = torch.zeros_like(logit).scatter_(-1, code, 1)\n",
       "        \n",
       "        return sample, code[..., 0], oneHot, logit\n",
       "\n",
       "\n",
       "class _multiCodebookDeQuantization(nn.Module):\n",
       "    \"\"\"\n",
       "    De-quantization module to reconstruct features from indices.\n",
       "    \"\"\"\n",
       "    \n",
       "    def __init__(self, codebook: nn.Parameter):\n",
       "        super().__init__()\n",
       "        self._m, self._k, self._d = codebook.shape\n",
       "        self._codebook = codebook  # Shared with quantization module\n",
       "    \n",
       "    def forward(self, sample: torch.Tensor) -> torch.Tensor:\n",
       "        \"\"\"\n",
       "        Reconstruct features from soft assignment.\n",
       "        \n",
       "        Args:\n",
       "            sample: [n, m, k] soft assignment\n",
       "        \n",
       "        Returns:\n",
       "            reconstructed: [n, c] reconstructed features\n",
       "        \"\"\"\n",
       "        n, _, _ = sample.shape\n",
       "        # Weighted sum: [n, m, k] Ã— [m, k, d] â†’ [n, m, d] â†’ [n, c]\n",
       "        return torch.einsum(\"nmk,mkd->nmd\", sample, self._codebook).reshape(n, -1)\n",
       "\n",
       "\n",
       "class UMGMQuantizer(nn.Module):\n",
       "    \"\"\"\n",
       "    Unified Multi-Granularity Multi-level Quantizer.\n",
       "    \n",
       "    This is the main codebook module used in QuantV2X.\n",
       "    \"\"\"\n",
       "    \n",
       "    def __init__(self, channel: int, m: int, k: int, \n",
       "                 permutationRate: float, components: dict):\n",
       "        \"\"\"\n",
       "        Args:\n",
       "            channel: Total number of feature channels (e.g., 64)\n",
       "            m: Number of segments (e.g., 1)\n",
       "            k: Dictionary size per segment (e.g., 128)\n",
       "            permutationRate: Random perturbation rate\n",
       "            components: Dictionary of component builders\n",
       "        \"\"\"\n",
       "        super().__init__()\n",
       "        \n",
       "        # Initialize codebook with SmallInit\n",
       "        # From \"Transformers without Tears\" (https://arxiv.org/pdf/1910.05895.pdf)\n",
       "        codebook = nn.Parameter(\n",
       "            nn.init.normal_(\n",
       "                torch.empty(m, k, channel // m),\n",
       "                std=math.sqrt(2 / (5 * channel / m))\n",
       "            )\n",
       "        )\n",
       "        \n",
       "        # Create quantizer and dequantizer\n",
       "        self.quantizer = _multiCodebookQuantization(codebook, permutationRate)\n",
       "        self.dequantizer = _multiCodebookDeQuantization(codebook)\n",
       "        \n",
       "        # Frequency tracking for codebook reassignment (EMA)\n",
       "        self.ema = 0.9\n",
       "        self._freqEMA = nn.Parameter(\n",
       "            torch.ones(m, k) / k, \n",
       "            requires_grad=False\n",
       "        )\n",
       "    \n",
       "    def forward(self, x: torch.Tensor):\n",
       "        \"\"\"\n",
       "        Args:\n",
       "            x: [B, C, H, W] input features\n",
       "        \n",
       "        Returns:\n",
       "            quantized: [B, C, H, W] quantized features\n",
       "            indices: [B, m, H, W] codebook indices (for transmission)\n",
       "            reconstruction_loss: MSE between input and quantized\n",
       "        \"\"\"\n",
       "        B, C, H, W = x.shape\n",
       "        \n",
       "        # Flatten spatial dimensions: [B, C, H, W] â†’ [B*H*W, C]\n",
       "        x_flat = x.permute(0, 2, 3, 1).reshape(-1, C)\n",
       "        \n",
       "        # Quantize\n",
       "        sample, code, oneHot, logit = self.quantizer(x_flat)\n",
       "        \n",
       "        # Dequantize\n",
       "        x_recon = self.dequantizer(sample)\n",
       "        \n",
       "        # Reshape back: [B*H*W, C] â†’ [B, C, H, W]\n",
       "        x_recon = x_recon.view(B, H, W, C).permute(0, 3, 1, 2)\n",
       "        indices = code.view(B, -1, H, W)  # [B, m, H, W]\n",
       "        \n",
       "        # Compute reconstruction loss\n",
       "        recon_loss = F.mse_loss(x_recon, x)\n",
       "        \n",
       "        return x_recon, indices, recon_loss\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display codebook implementation\n",
    "codebook_impl = '''\n",
    "```python\n",
    "# opencood/models/sub_modules/codebook.py (Core quantization logic)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class _multiCodebookQuantization(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-codebook vector quantization module.\n",
    "    \n",
    "    Args:\n",
    "        codebook: [m, k, d] learnable codebook\n",
    "            m: number of segments\n",
    "            k: dict_size (number of codewords per segment)\n",
    "            d: channel dimension per segment (total_channels // m)\n",
    "        permutationRate: probability of random perturbation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, codebook: nn.Parameter, permutationRate: float = 0.0):\n",
    "        super().__init__()\n",
    "        self._m, self._k, self._d = codebook.shape  # e.g., [1, 128, 64]\n",
    "        self._codebook = codebook  # Learnable!\n",
    "        self._scale = math.sqrt(self._k)\n",
    "        self._temperature = nn.Parameter(torch.ones((self._m, 1)))\n",
    "    \n",
    "    def _distance(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute L2 distance between input features and all codewords.\n",
    "        \n",
    "        Args:\n",
    "            x: [n, c] input features (c = m * d)\n",
    "        \n",
    "        Returns:\n",
    "            distance: [n, m, k] distances to each codeword\n",
    "        \"\"\"\n",
    "        n, _ = x.shape\n",
    "        # Reshape: [n, c] â†’ [n, m, d]\n",
    "        x = x.reshape(n, self._m, self._d)\n",
    "        \n",
    "        # Squared norm of input: [n, m, 1]\n",
    "        x2 = (x ** 2).sum(2, keepdim=True)\n",
    "        \n",
    "        # Squared norm of codebook: [m, k]\n",
    "        c2 = (self._codebook ** 2).sum(-1, keepdim=False)\n",
    "        \n",
    "        # Inner product: [n, m, k]\n",
    "        inter = torch.einsum(\"nmd,mkd->nmk\", x, self._codebook)\n",
    "        \n",
    "        # L2 distance: ||x - c||^2 = ||x||^2 + ||c||^2 - 2<x, c>\n",
    "        distance = x2 + c2 - 2 * inter\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode input features to codebook indices.\n",
    "        \n",
    "        Args:\n",
    "            x: [n, c] input features\n",
    "        \n",
    "        Returns:\n",
    "            code: [n, m] codebook indices\n",
    "        \"\"\"\n",
    "        distance = self._distance(x)  # [n, m, k]\n",
    "        code = distance.argmin(-1)     # [n, m] - nearest codeword\n",
    "        return code\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass with Gumbel-Softmax for differentiable sampling.\n",
    "        \n",
    "        Args:\n",
    "            x: [n, c] input features\n",
    "        \n",
    "        Returns:\n",
    "            sample: [n, m, k] soft assignment (differentiable)\n",
    "            code: [n, m] hard assignment (indices)\n",
    "            oneHot: [n, m, k] one-hot encoding\n",
    "            logit: [n, m, k] logits for probability distribution\n",
    "        \"\"\"\n",
    "        # Compute logits (negative distance)\n",
    "        logit = -1 * self._distance(x) / self._scale\n",
    "        \n",
    "        # Gumbel-Softmax sampling (differentiable!)\n",
    "        sample = gumbelSoftmax(logit, temperature=1.0, hard=True)\n",
    "        \n",
    "        # Get hard assignment\n",
    "        code = logit.argmax(-1, keepdim=True)  # [n, m, 1]\n",
    "        \n",
    "        # One-hot encoding\n",
    "        oneHot = torch.zeros_like(logit).scatter_(-1, code, 1)\n",
    "        \n",
    "        return sample, code[..., 0], oneHot, logit\n",
    "\n",
    "\n",
    "class _multiCodebookDeQuantization(nn.Module):\n",
    "    \"\"\"\n",
    "    De-quantization module to reconstruct features from indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, codebook: nn.Parameter):\n",
    "        super().__init__()\n",
    "        self._m, self._k, self._d = codebook.shape\n",
    "        self._codebook = codebook  # Shared with quantization module\n",
    "    \n",
    "    def forward(self, sample: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reconstruct features from soft assignment.\n",
    "        \n",
    "        Args:\n",
    "            sample: [n, m, k] soft assignment\n",
    "        \n",
    "        Returns:\n",
    "            reconstructed: [n, c] reconstructed features\n",
    "        \"\"\"\n",
    "        n, _, _ = sample.shape\n",
    "        # Weighted sum: [n, m, k] Ã— [m, k, d] â†’ [n, m, d] â†’ [n, c]\n",
    "        return torch.einsum(\"nmk,mkd->nmd\", sample, self._codebook).reshape(n, -1)\n",
    "\n",
    "\n",
    "class UMGMQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified Multi-Granularity Multi-level Quantizer.\n",
    "    \n",
    "    This is the main codebook module used in QuantV2X.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channel: int, m: int, k: int, \n",
    "                 permutationRate: float, components: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channel: Total number of feature channels (e.g., 64)\n",
    "            m: Number of segments (e.g., 1)\n",
    "            k: Dictionary size per segment (e.g., 128)\n",
    "            permutationRate: Random perturbation rate\n",
    "            components: Dictionary of component builders\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize codebook with SmallInit\n",
    "        # From \"Transformers without Tears\" (https://arxiv.org/pdf/1910.05895.pdf)\n",
    "        codebook = nn.Parameter(\n",
    "            nn.init.normal_(\n",
    "                torch.empty(m, k, channel // m),\n",
    "                std=math.sqrt(2 / (5 * channel / m))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create quantizer and dequantizer\n",
    "        self.quantizer = _multiCodebookQuantization(codebook, permutationRate)\n",
    "        self.dequantizer = _multiCodebookDeQuantization(codebook)\n",
    "        \n",
    "        # Frequency tracking for codebook reassignment (EMA)\n",
    "        self.ema = 0.9\n",
    "        self._freqEMA = nn.Parameter(\n",
    "            torch.ones(m, k) / k, \n",
    "            requires_grad=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, C, H, W] input features\n",
    "        \n",
    "        Returns:\n",
    "            quantized: [B, C, H, W] quantized features\n",
    "            indices: [B, m, H, W] codebook indices (for transmission)\n",
    "            reconstruction_loss: MSE between input and quantized\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Flatten spatial dimensions: [B, C, H, W] â†’ [B*H*W, C]\n",
    "        x_flat = x.permute(0, 2, 3, 1).reshape(-1, C)\n",
    "        \n",
    "        # Quantize\n",
    "        sample, code, oneHot, logit = self.quantizer(x_flat)\n",
    "        \n",
    "        # Dequantize\n",
    "        x_recon = self.dequantizer(sample)\n",
    "        \n",
    "        # Reshape back: [B*H*W, C] â†’ [B, C, H, W]\n",
    "        x_recon = x_recon.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        indices = code.view(B, -1, H, W)  # [B, m, H, W]\n",
    "        \n",
    "        # Compute reconstruction loss\n",
    "        recon_loss = F.mse_loss(x_recon, x)\n",
    "        \n",
    "        return x_recon, indices, recon_loss\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(codebook_impl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Freezing Mechanism in Stage 2\n",
    "\n",
    "The **critical difference** in Stage 2 is that we freeze all parameters except the codebook.\n",
    "\n",
    "Here's the **actual code** from `opencood/tools/train_stage2.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display actual freezing code from train_stage2.py\n",
    "freezing_code = '''\n",
    "```python\n",
    "# opencood/tools/train_stage2.py (Parameter freezing logic)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from opencood.tools import train_utils\n",
    "\n",
    "def main():\n",
    "    # ... (load config, create dataset) ...\n",
    "    \n",
    "    # ============================================================\n",
    "    # 1. Create model WITH codebook\n",
    "    # ============================================================\n",
    "    print(\"Building model with codebook...\")\n",
    "    model = train_utils.create_model(hypes)  # HeterPyramidCollabCodebookMC\n",
    "    device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. Load Stage 1 checkpoint (pretrained weights)\n",
    "    # ============================================================\n",
    "    stage1_checkpoint = args.stage1_model\n",
    "    print(f\"Loading Stage 1 checkpoint from: {stage1_checkpoint}\")\n",
    "    \n",
    "    stage1_dict = torch.load(stage1_checkpoint, map_location=\\'cpu\\')\n",
    "    \n",
    "    # Load pretrained weights (ignoring codebook since it doesn\\'t exist in Stage 1)\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in stage1_dict.items() \n",
    "                       if k in model_dict and \\'codebook\\' not in k}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    \n",
    "    print(f\"Loaded {len(pretrained_dict)} / {len(stage1_dict)} parameters from Stage 1\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. FREEZE all parameters\n",
    "    # ============================================================\n",
    "    print(\"\\nFreezing all parameters...\")\n",
    "    model.eval()  # Set to eval mode\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)  # Freeze everything!\n",
    "    \n",
    "    # ============================================================\n",
    "    # 4. UNFREEZE codebook parameters only\n",
    "    # ============================================================\n",
    "    print(\"Unfreezing codebook parameters...\")\n",
    "    model.codebook.train()  # Set codebook to train mode\n",
    "    for p in model.codebook.parameters():\n",
    "        p.requires_grad_(True)  # Enable gradients for codebook!\n",
    "    \n",
    "    # ============================================================\n",
    "    # 5. Print trainable parameters\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Trainable Parameters (Stage 2):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_params = 0\n",
    "    total_trainable = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            print(f\"  âœ“ {name}: {param.data.shape}\")\n",
    "            total_trainable += param.numel()\n",
    "        else:\n",
    "            # Only print first few frozen params\n",
    "            if total_params - total_trainable < 1000:\n",
    "                print(f\"  âœ— {name}: {param.data.shape} [FROZEN]\")\n",
    "    \n",
    "    print(f\"\\nTotal parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {total_trainable:,} ({total_trainable/total_params*100:.2f}%)\")\n",
    "    print(f\"Frozen parameters:    {total_params-total_trainable:,} ({(total_params-total_trainable)/total_params*100:.2f}%)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ============================================================\n",
    "    # 6. Create optimizer ONLY for codebook parameters\n",
    "    # ============================================================\n",
    "    codebook_params = [p for n, p in model.named_parameters() \n",
    "                       if p.requires_grad]\n",
    "    \n",
    "    print(f\"\\nOptimizer will update {len(codebook_params)} parameter groups\")\n",
    "    optimizer = torch.optim.Adam(codebook_params, lr=hypes[\\'optimizer\\'][\\'lr\\'])\n",
    "    \n",
    "    # ... (rest of training loop) ...\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(freezing_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Training Commands\n",
    "\n",
    "```bash\n",
    "# IMPORTANT: Must provide --stage1_model checkpoint!\n",
    "python ./opencood/tools/train_stage2.py \\\n",
    "    --hypes_yaml ./opencood/hypes_yaml/v2x_real/Codebook/stage2/lidar_pyramid_stage2.yaml \\\n",
    "    --stage1_model opencood/logs/stage1_model_YYYY_MM_DD_HH_MM_SS/net_epoch_best.pth\n",
    "\n",
    "# Multi-GPU version\n",
    "CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch \\\n",
    "    --nproc_per_node=2 --use_env \\\n",
    "    ./opencood/tools/train_stage2.py \\\n",
    "    --hypes_yaml ./opencood/hypes_yaml/v2x_real/Codebook/stage2/lidar_pyramid_stage2.yaml \\\n",
    "    --stage1_model opencood/logs/stage1_model_YYYY_MM_DD_HH_MM_SS/net_epoch_best.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='stage3'></a>\n",
    "## 6. Stage 3: End-to-End Co-Training\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Stage 3 performs **end-to-end fine-tuning** of the entire model:\n",
    "1. **Load Stage 2 checkpoint** (pretrained model + trained codebook)\n",
    "2. **Unfreeze ALL parameters** (codebook + detector)\n",
    "3. **Joint optimization** with smaller learning rate\n",
    "4. **Achieve best performance** by co-adapting codebook and detector\n",
    "\n",
    "### Key Differences from Stage 2\n",
    "\n",
    "| Aspect | Stage 2 | Stage 3 |\n",
    "|--------|---------|----------|\n",
    "| Model | `heter_pyramid_collab_codebook_mc` | Same |\n",
    "| Training | **Codebook only** | **ALL parameters** |\n",
    "| Initialization | Load Stage 1 | **Load Stage 2** |\n",
    "| Batch Size | 8 | **4** (reduced) |\n",
    "| Learning Rate | 0.002 | **0.0002** (10x smaller) |\n",
    "| Epochs | 20 | **10** (fewer) |\n",
    "| LR Schedule | [15, 25] | **[5, 8]** (earlier decay) |\n",
    "| Special | - | `stage3_codebook_weight: 0.05` |\n",
    "\n",
    "### Why These Changes?\n",
    "\n",
    "1. **Smaller Batch Size (4 vs 8)**: \n",
    "   - More stable gradients during fine-tuning\n",
    "   - Prevents overfitting to specific batches\n",
    "\n",
    "2. **Smaller Learning Rate (0.0002 vs 0.002)**:\n",
    "   - Prevents destroying pretrained features\n",
    "   - Allows gentle co-adaptation\n",
    "\n",
    "3. **Fewer Epochs (10 vs 20)**:\n",
    "   - Already good initialization from Stage 2\n",
    "   - Prevents overfitting\n",
    "\n",
    "4. **Codebook Weight Regularization**:\n",
    "   - Balances detection loss and codebook compression\n",
    "   - Prevents codebook from degrading during joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Stage 3 unfreezing code\n",
    "stage3_code = '''\n",
    "```python\n",
    "# opencood/tools/train_stage3.py (Unfreezing logic)\n",
    "\n",
    "def main():\n",
    "    # ... (load config, create dataset) ...\n",
    "    \n",
    "    # ============================================================\n",
    "    # 1. Create model\n",
    "    # ============================================================\n",
    "    model = train_utils.create_model(hypes)  # HeterPyramidCollabCodebookMC\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. Load Stage 2 checkpoint (pretrained + codebook)\n",
    "    # ============================================================\n",
    "    stage2_checkpoint = args.stage2_model\n",
    "    print(f\"Loading Stage 2 checkpoint from: {stage2_checkpoint}\")\n",
    "    \n",
    "    stage2_dict = torch.load(stage2_checkpoint, map_location=\\'cpu\\')\n",
    "    model.load_state_dict(stage2_dict)  # Load everything!\n",
    "    \n",
    "    print(f\"Loaded complete model from Stage 2 (detector + codebook)\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. UNFREEZE all parameters\n",
    "    # ============================================================\n",
    "    print(\"\\nUnfreezing all parameters for end-to-end training...\")\n",
    "    model.train()  # Set entire model to train mode\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True  # Enable gradients for ALL parameters!\n",
    "    \n",
    "    # ============================================================\n",
    "    # 4. Verify all parameters are trainable\n",
    "    # ============================================================\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nTotal parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} (100%)\")\n",
    "    print(f\"Frozen parameters:    0\")\n",
    "    \n",
    "    assert total_params == trainable_params, \"Some parameters are still frozen!\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # 5. Create optimizer with SMALLER learning rate\n",
    "    # ============================================================\n",
    "    lr = hypes[\\'optimizer\\'][\\'lr\\']  # 0.0002 (10x smaller than Stage 1/2)\n",
    "    print(f\"\\nLearning rate: {lr} (10x smaller for fine-tuning)\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # ============================================================\n",
    "    # 6. Learning rate scheduler with EARLIER decay\n",
    "    # ============================================================\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[5, 8],  # Decay at epochs 5 and 8 (earlier than Stage 1/2)\n",
    "        gamma=0.1\n",
    "    )\n",
    "    \n",
    "    # ... (training loop) ...\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(stage3_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Training Commands\n",
    "\n",
    "```bash\n",
    "# IMPORTANT: Must provide --stage2_model checkpoint!\n",
    "python ./opencood/tools/train_stage3.py \\\n",
    "    --hypes_yaml ./opencood/hypes_yaml/v2x_real/Codebook/stage3/lidar_pyramid_stage3.yaml \\\n",
    "    --stage2_model opencood/logs/stage2_model_YYYY_MM_DD_HH_MM_SS/net_epoch_best.pth\n",
    "\n",
    "# Multi-GPU version\n",
    "CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch \\\n",
    "    --nproc_per_node=2 --use_env \\\n",
    "    ./opencood/tools/train_stage3.py \\\n",
    "    --hypes_yaml ./opencood/hypes_yaml/v2x_real/Codebook/stage3/lidar_pyramid_stage3.yaml \\\n",
    "    --stage2_model opencood/logs/stage2_model_YYYY_MM_DD_HH_MM_SS/net_epoch_best.pth\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "```\n",
    "opencood/logs/\n",
    "â””â”€â”€ stage3_model_YYYY_MM_DD_HH_MM_SS/\n",
    "    â”œâ”€â”€ config.yaml\n",
    "    â”œâ”€â”€ net_epoch_1.pth\n",
    "    â”œâ”€â”€ ...\n",
    "    â”œâ”€â”€ net_epoch_10.pth\n",
    "    â””â”€â”€ net_epoch_best.pth       # â† Final model for deployment!\n",
    "```\n",
    "\n",
    "### Training Time\n",
    "\n",
    "- **Single GPU**: ~1 day (10 epochs with smaller batch size)\n",
    "- **2 GPUs**: ~12 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part-iii'></a>\n",
    "# Part III: Post-Training Quantization (PTQ)\n",
    "\n",
    "<a id='ptq-theory'></a>\n",
    "## 7. PTQ Theory & Implementation\n",
    "\n",
    "### What is Post-Training Quantization?\n",
    "\n",
    "**Post-Training Quantization (PTQ)** converts a trained FP32 model to lower precision (INT8/INT4) **without retraining**:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  FP32 Model (Stage 3 output)                                    â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â€¢ Weights: FP32 (4 bytes per parameter)                       â”‚\n",
    "â”‚  â€¢ Activations: FP32 (4 bytes per value)                       â”‚\n",
    "â”‚  â€¢ Operations: FP32 multiply-add                               â”‚                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“ PTQ (W8A8)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  INT8 Model (After PTQ)                                         â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  â€¢ Weights: INT8 (1 byte per parameter)                        â”‚\n",
    "â”‚  â€¢ Activations: INT8 (1 byte per value)                        â”‚\n",
    "â”‚  â€¢ Operations: INT8 multiply-add                               |      \n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Quantization Formula\n",
    "\n",
    "For a given FP32 value $x$, quantization to INT8 is:\n",
    "\n",
    "$$\n",
    "x_{\\text{quant}} = \\text{clamp}\\left(\\text{round}\\left(\\frac{x}{s}\\right) + z, 0, 255\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $s$ = scale factor (calibrated from data)\n",
    "- $z$ = zero-point (often 0 for symmetric quantization)\n",
    "\n",
    "**Dequantization** (to recover approximate FP32):\n",
    "\n",
    "$$\n",
    "x_{\\text{dequant}} = (x_{\\text{quant}} - z) \\times s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='ptq-pipeline'></a>\n",
    "## 8. Quantization Pipeline\n",
    "\n",
    "### PTQ Workflow\n",
    "\n",
    "```\n",
    "1. Load FP32 Model (Stage 3)\n",
    "   â†“\n",
    "2. Replace layers with quantized versions\n",
    "   â€¢ nn.Conv2d â†’ QuantConv2d\n",
    "   â€¢ nn.Linear â†’ QuantLinear\n",
    "   â†“\n",
    "3. Calibration Phase\n",
    "   â€¢ Forward pass with calibration data\n",
    "   â€¢ Collect activation statistics\n",
    "   â€¢ Compute optimal scales\n",
    "   â†“\n",
    "4. Weight Optimization\n",
    "   â€¢ Minimize reconstruction error\n",
    "   â€¢ Iterate for N steps\n",
    "   â†“\n",
    "5. Validation & Inference\n",
    "   â€¢ Run on test set\n",
    "   â€¢ Compare with FP32 baseline\n",
    "```\n",
    "\n",
    "### PTQ Command\n",
    "\n",
    "```bash\n",
    "# W8A8 Quantization\n",
    "python opencood/tools/inference_mc_quant.py \\\n",
    "    --model_dir opencood/logs/stage3_model_YYYY_MM_DD_HH_MM_SS \\\n",
    "    --fusion_method intermediate \\\n",
    "    --num_cali_batches 16 \\\n",
    "    --n_bits_w 8 \\\n",
    "    --n_bits_a 8 \\\n",
    "    --iters_w 5000\n",
    "```\n",
    "\n",
    "### PTQ Parameters\n",
    "\n",
    "| Parameter | Description | Recommended |\n",
    "|-----------|-------------|-------------|\n",
    "| `--num_cali_batches` | Number of batches for calibration | 0-32 |\n",
    "| `--n_bits_w` | Weight bitwidth | 4, 8 |\n",
    "| `--n_bits_a` | Activation bitwidth | 4, 8 |\n",
    "| `--iters_w` | Weight optimization iterations | 2000-10000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "# opencood/tools/inference_mc_quant.py (Key sections)\n",
       "\n",
       "from opencood.utils import quant_utils\n",
       "from opencood.utils.quant_model import QuantConv2d, QuantLinear\n",
       "\n",
       "def main():\n",
       "    # ============================================================\n",
       "    # 1. Load FP32 model\n",
       "    # ============================================================\n",
       "    fp_model = load_model(args.model_dir)\n",
       "    fp_model.eval()\n",
       "    \n",
       "    print(f\"Loaded FP32 model: {sum(p.numel() for p in fp_model.parameters()):,} parameters\")\n",
       "    \n",
       "    # ============================================================\n",
       "    # 2. Build quantization parameters\n",
       "    # ============================================================\n",
       "    wq_params = {\n",
       "        'n_bits': args.n_bits_w,        # Weight bitwidth (e.g., 8)\n",
       "        'channel_wise': True,          # Per-channel quantization\n",
       "        'scale_method': 'mse'         # MSE-based scale initialization\n",
       "    }\n",
       "    \n",
       "    aq_params = {\n",
       "        'n_bits': args.n_bits_a,        # Activation bitwidth (e.g., 8)\n",
       "        'channel_wise': False,         # Per-layer quantization\n",
       "        'scale_method': 'mse',\n",
       "        'leaf_param': True\n",
       "    }\n",
       "    \n",
       "    # ============================================================\n",
       "    # 3. Replace layers with quantized versions\n",
       "    # ============================================================\n",
       "    print(\"\n",
       "Replacing layers with quantized versions...\")\n",
       "    qnn = quant_utils.QuantModel(\n",
       "        model=fp_model,\n",
       "        weight_quant_params=wq_params,\n",
       "        act_quant_params=aq_params\n",
       "    )\n",
       "    qnn.cuda()\n",
       "    qnn.eval()\n",
       "    \n",
       "    # ============================================================\n",
       "    # 4. Calibration Phase\n",
       "    # ============================================================\n",
       "    print(f\"\n",
       "Calibrating with {args.num_cali_batches} batches...\")\n",
       "    \n",
       "    # Disable quantization for calibration\n",
       "    qnn.set_quant_state(False, False)\n",
       "    \n",
       "    # Forward pass to collect statistics\n",
       "    with torch.no_grad():\n",
       "        for i, batch in enumerate(calibration_loader):\n",
       "            if i >= args.num_cali_batches:\n",
       "                break\n",
       "            _ = qnn(batch)\n",
       "            print(f\"  Calibration: {i+1}/{args.num_cali_batches}\", end='\r",
       "')\n",
       "    \n",
       "    # ============================================================\n",
       "    # 5. Initialize quantization parameters\n",
       "    # ============================================================\n",
       "    print(\"\n",
       "\n",
       "Initializing quantization parameters...\")\n",
       "    qnn.set_quant_state(True, True)  # Enable weight + activation quantization\n",
       "    \n",
       "    # ============================================================\n",
       "    # 6. Weight Optimization (BRECQ)\n",
       "    # ============================================================\n",
       "    print(f\"\n",
       "Optimizing weights for {args.iters_w} iterations...\")\n",
       "    \n",
       "    for name, module in qnn.named_modules():\n",
       "        if isinstance(module, (QuantConv2d, QuantLinear)):\n",
       "            # Optimize scale for this layer\n",
       "            optimize_layer_scale(\n",
       "                module,\n",
       "                calibration_loader,\n",
       "                iters=args.iters_w,\n",
       "                lr=4e-5\n",
       "            )\n",
       "    \n",
       "    print(\"\n",
       "Quantization complete!\")\n",
       "    \n",
       "    # ============================================================\n",
       "    # 7. Inference & Evaluation\n",
       "    # ============================================================\n",
       "    print(\"\n",
       "Running quantized inference...\")\n",
       "    results = evaluate(qnn, test_loader)\n",
       "    \n",
       "    print(f\"\n",
       "Quantized Model Results:\")\n",
       "    print(f\"  AP: {results['AP']:.4f}\")\n",
       "    print(f\"  Inference Time: {results['time']:.2f} ms/frame\")\n",
       "\n",
       "\n",
       "def optimize_layer_scale(layer, data_loader, iters, lr):\n",
       "    \"\"\"\n",
       "    Optimize quantization scale for a single layer.\n",
       "    \n",
       "    Minimizes: MSE(output_fp32, output_quant)\n",
       "    \"\"\"\n",
       "    optimizer = torch.optim.Adam([layer.weight_quantizer.scale], lr=lr)\n",
       "    \n",
       "    for i in range(iters):\n",
       "        # Get FP32 output\n",
       "        layer.set_quant_state(False, False)\n",
       "        out_fp = layer(input_data)\n",
       "        \n",
       "        # Get quantized output\n",
       "        layer.set_quant_state(True, True)\n",
       "        out_quant = layer(input_data)\n",
       "        \n",
       "        # Compute reconstruction loss\n",
       "        loss = F.mse_loss(out_quant, out_fp.detach())\n",
       "        \n",
       "        # Optimize\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "        \n",
       "        if i % 1000 == 0:\n",
       "            print(f\"    Iter {i}/{iters}, Loss: {loss.item():.6f}\")\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display PTQ implementation code\n",
    "ptq_code = '''\n",
    "```python\n",
    "# opencood/tools/inference_mc_quant.py (Key sections)\n",
    "\n",
    "from opencood.utils import quant_utils\n",
    "from opencood.utils.quant_model import QuantConv2d, QuantLinear\n",
    "\n",
    "def main():\n",
    "    # ============================================================\n",
    "    # 1. Load FP32 model\n",
    "    # ============================================================\n",
    "    fp_model = load_model(args.model_dir)\n",
    "    fp_model.eval()\n",
    "    \n",
    "    print(f\"Loaded FP32 model: {sum(p.numel() for p in fp_model.parameters()):,} parameters\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. Build quantization parameters\n",
    "    # ============================================================\n",
    "    wq_params = {\n",
    "        \\'n_bits\\': args.n_bits_w,        # Weight bitwidth (e.g., 8)\n",
    "        \\'channel_wise\\': True,          # Per-channel quantization\n",
    "        \\'scale_method\\': \\'mse\\'         # MSE-based scale initialization\n",
    "    }\n",
    "    \n",
    "    aq_params = {\n",
    "        \\'n_bits\\': args.n_bits_a,        # Activation bitwidth (e.g., 8)\n",
    "        \\'channel_wise\\': False,         # Per-layer quantization\n",
    "        \\'scale_method\\': \\'mse\\',\n",
    "        \\'leaf_param\\': True\n",
    "    }\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. Replace layers with quantized versions\n",
    "    # ============================================================\n",
    "    print(\"\\nReplacing layers with quantized versions...\")\n",
    "    qnn = quant_utils.QuantModel(\n",
    "        model=fp_model,\n",
    "        weight_quant_params=wq_params,\n",
    "        act_quant_params=aq_params\n",
    "    )\n",
    "    qnn.cuda()\n",
    "    qnn.eval()\n",
    "    \n",
    "    # ============================================================\n",
    "    # 4. Calibration Phase\n",
    "    # ============================================================\n",
    "    print(f\"\\nCalibrating with {args.num_cali_batches} batches...\")\n",
    "    \n",
    "    # Disable quantization for calibration\n",
    "    qnn.set_quant_state(False, False)\n",
    "    \n",
    "    # Forward pass to collect statistics\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(calibration_loader):\n",
    "            if i >= args.num_cali_batches:\n",
    "                break\n",
    "            _ = qnn(batch)\n",
    "            print(f\"  Calibration: {i+1}/{args.num_cali_batches}\", end=\\'\\r\\')\n",
    "    \n",
    "    # ============================================================\n",
    "    # 5. Initialize quantization parameters\n",
    "    # ============================================================\n",
    "    print(\"\\n\\nInitializing quantization parameters...\")\n",
    "    qnn.set_quant_state(True, True)  # Enable weight + activation quantization\n",
    "    \n",
    "    # ============================================================\n",
    "    # 6. Weight Optimization (BRECQ)\n",
    "    # ============================================================\n",
    "    print(f\"\\nOptimizing weights for {args.iters_w} iterations...\")\n",
    "    \n",
    "    for name, module in qnn.named_modules():\n",
    "        if isinstance(module, (QuantConv2d, QuantLinear)):\n",
    "            # Optimize scale for this layer\n",
    "            optimize_layer_scale(\n",
    "                module,\n",
    "                calibration_loader,\n",
    "                iters=args.iters_w,\n",
    "                lr=4e-5\n",
    "            )\n",
    "    \n",
    "    print(\"\\nQuantization complete!\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 7. Inference & Evaluation\n",
    "    # ============================================================\n",
    "    print(\"\\nRunning quantized inference...\")\n",
    "    results = evaluate(qnn, test_loader)\n",
    "    \n",
    "    print(f\"\\nQuantized Model Results:\")\n",
    "    print(f\"  AP: {results[\\'AP\\']:.4f}\")\n",
    "    print(f\"  Inference Time: {results[\\'time\\']:.2f} ms/frame\")\n",
    "\n",
    "\n",
    "def optimize_layer_scale(layer, data_loader, iters, lr):\n",
    "    \"\"\"\n",
    "    Optimize quantization scale for a single layer.\n",
    "    \n",
    "    Minimizes: MSE(output_fp32, output_quant)\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam([layer.weight_quantizer.scale], lr=lr)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        # Get FP32 output\n",
    "        layer.set_quant_state(False, False)\n",
    "        out_fp = layer(input_data)\n",
    "        \n",
    "        # Get quantized output\n",
    "        layer.set_quant_state(True, True)\n",
    "        out_quant = layer(input_data)\n",
    "        \n",
    "        # Compute reconstruction loss\n",
    "        loss = F.mse_loss(out_quant, out_fp.detach())\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"    Iter {i}/{iters}, Loss: {loss.item():.6f}\")\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(ptq_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ptq-workflow'></a>\n",
    "### Detailed PTQ Workflow in `inference_mc_quant.py`\n",
    "\n",
    "Let's examine the **complete workflow** of the PTQ inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display complete PTQ workflow structure\n",
    "ptq_workflow = '''\n",
    "```python\n",
    "# opencood/tools/inference_mc_quant.py - Complete Workflow Structure\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 1: Import Required Modules\n",
    "# ============================================================\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from opencood.quant import (\n",
    "    QuantModel,                    # Wrapper to make model quantizable\n",
    "    block_reconstruction,          # Reconstruct blocks (ResNet, etc.)\n",
    "    layer_reconstruction,          # Reconstruct layers (Conv2d, Linear)\n",
    "    pyramid_reconstruction,        # Reconstruct pyramid fusion\n",
    "    encoder_reconstruction,        # Reconstruct PointPillar encoder\n",
    "    set_weight_quantize_params,    # Initialize weight quantizers\n",
    ")\n",
    "from opencood.quant.quant_block import (\n",
    "    QuantBaseBEVBackbone,\n",
    "    QuantPyramidFusion,\n",
    "    QuantPFNLayer,\n",
    "    QuantDownsampleConv,\n",
    "    # ... (other quantized block types)\n",
    ")\n",
    "from icecream import ic  # For model printing\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 2: Load Full-Precision Model (Stage 3 Checkpoint)\n",
    "# ============================================================\n",
    "'''\n",
    "print(\\'Creating Model\\')\n",
    "trained_model = train_utils.create_model(hypes)  # HeterPyramidCollabCodebookMC\n",
    "\n",
    "print(\\'Loading Model from checkpoint\\')\n",
    "saved_path = opt.model_dir\n",
    "resume_epoch, trained_model = train_utils.load_saved_model(saved_path, trained_model)\n",
    "print(f\"resume from {resume_epoch} epoch.\")\n",
    "\n",
    "trained_model.cuda()\n",
    "trained_model.eval()\n",
    "\n",
    "# Create a copy for FP32 baseline\n",
    "fp_model = copy.deepcopy(trained_model)\n",
    "fp_model.cuda()\n",
    "fp_model.eval()\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 3: Define Quantization Parameters\n",
    "# ============================================================\n",
    "'''\n",
    "# Weight quantization parameters\n",
    "wq_params = {\n",
    "    \\'n_bits\\': opt.n_bits_w,           # e.g., 8 for W8\n",
    "    \\'channel_wise\\': opt.channel_wise, # True for per-channel\n",
    "    \\'scale_method\\': opt.init_wmode   # \\'mse\\' or \\'minmax\\'\n",
    "}\n",
    "\n",
    "# Activation quantization parameters\n",
    "aq_params = {\n",
    "    \\'n_bits\\': opt.n_bits_a,           # e.g., 8 for A8\n",
    "    \\'channel_wise\\': False,           # False for per-layer\n",
    "    \\'scale_method\\': opt.init_amode,  # \\'mse\\' or \\'minmax\\'\n",
    "    \\'leaf_param\\': True,\n",
    "    \\'prob\\': opt.prob\n",
    "}\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 4: Wrap Models with QuantModel\n",
    "# ============================================================\n",
    "'''\n",
    "# FP32 model (for comparison, no quantization)\n",
    "fp_model = QuantModel(\n",
    "    model=fp_model,\n",
    "    weight_quant_params=wq_params,\n",
    "    act_quant_params=aq_params,\n",
    "    is_fusing=False  # Don\\'t fuse BatchNorm for FP32\n",
    ")\n",
    "fp_model.cuda()\n",
    "fp_model.eval()\n",
    "fp_model.set_quant_state(False, False)  # Disable quantization\n",
    "\n",
    "# Quantized model\n",
    "qt_model = QuantModel(\n",
    "    model=trained_model,\n",
    "    weight_quant_params=wq_params,\n",
    "    act_quant_params=aq_params\n",
    ")  # is_fusing=True by default - fuses BatchNorm\n",
    "qt_model.cuda()\n",
    "qt_model.eval()\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 5: Print Model Structures Using ic()\n",
    "# ============================================================\n",
    "'''\n",
    "print(\\'the fp model is below!\\')\n",
    "ic(fp_model)  # Shows FP32 model structure\n",
    "\n",
    "# Disable quantization for output layers (detection heads)\n",
    "qt_model.disable_network_output_quantization()\n",
    "\n",
    "print(\\'the quantized model is below!\\')\n",
    "ic(qt_model)  # Shows quantized model structure with QuantModule layers\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 6: Prepare Calibration Data\n",
    "# ============================================================\n",
    "'''\n",
    "cali_data = get_train_samples(train_loader, num_batches=opt.num_cali_batches)\n",
    "print(f\"Collected {len(cali_data)} calibration batches\")\n",
    "\n",
    "# Kwargs for reconstruction\n",
    "kwargs = dict(\n",
    "    cali_data=cali_data,\n",
    "    iters=opt.iters_w,             # e.g., 5000\n",
    "    weight=opt.weight,             # Rounding loss weight\n",
    "    b_range=(opt.b_start, opt.b_end),  # Temperature range\n",
    "    warmup=opt.warmup,             # Warmup period\n",
    "    opt_mode=\\'mse\\',               # Optimization mode\n",
    "    lr=opt.lr,                     # Learning rate for LSQ\n",
    "    input_prob=opt.input_prob,\n",
    "    keep_gpu=not opt.keep_cpu,\n",
    "    lamb_r=opt.lamb_r,             # KL divergence weight\n",
    "    T=opt.T,                       # Temperature for KD\n",
    "    bn_lr=opt.bn_lr,               # BN learning rate\n",
    "    lamb_c=opt.lamb_c              # BN constraint weight\n",
    ")\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 7: Initialize Weight Quantizers\n",
    "# ============================================================\n",
    "'''\n",
    "set_weight_quantize_params(qt_model)\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 8: Recursive Block/Layer Reconstruction\n",
    "# ============================================================\n",
    "'''\n",
    "def recon_model(qt: nn.Module, fp: nn.Module):\n",
    "    \"\"\"\n",
    "    Recursively reconstruct quantized model to match FP32 outputs.\n",
    "    \n",
    "    For each module type, use specialized reconstruction:\n",
    "    - QuantModule (Conv2d, Linear): layer_reconstruction\n",
    "    - QuantPyramidFusion: pyramid_reconstruction  \n",
    "    - QuantResNetBEVBackbone: block_reconstruction\n",
    "    - QuantPFNLayer: encoder_reconstruction\n",
    "    - etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    for (name, module), (_, fp_module) in zip(qt.named_children(), fp.named_children()):\n",
    "        if isinstance(module, QuantModule):\n",
    "            print(f\\'Reconstruction for layer {name}\\')\n",
    "            layer_reconstruction(qt_model, fp_model, module, fp_module, **kwargs)\n",
    "        \n",
    "        elif isinstance(module, QuantPyramidFusion):\n",
    "            print(f\\'Reconstruction for pyramid fusion block {name}\\')\n",
    "            pyramid_reconstruction(qt_model, fp_model, module, fp_module, **kwargs)\n",
    "        \n",
    "        elif isinstance(module, (QuantResNetBEVBackbone, QuantDownsampleConv, QuantBaseBEVBackbone)):\n",
    "            print(f\\'Reconstruction for block {name}\\')\n",
    "            block_reconstruction(qt_model, fp_model, module, fp_module, **kwargs)\n",
    "        \n",
    "        elif isinstance(module, QuantPFNLayer):\n",
    "            print(f\\'Reconstruction for PointPillar PFN {name}\\')\n",
    "            encoder_reconstruction(qt_model, fp_model, module, fp_module, **kwargs)\n",
    "        \n",
    "        else:\n",
    "            # Recursively process sub-modules\n",
    "            recon_model(module, fp_module)\n",
    "\n",
    "# Start reconstruction\n",
    "print(\"\\\\nStarting block/layer-wise reconstruction...\")\n",
    "recon_model(qt_model, fp_model)\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 9: Enable Quantization\n",
    "# ============================================================\n",
    "'''\n",
    "qt_model.set_quant_state(weight_quant=True, act_quant=True)\n",
    "print(\\'Quantization is done!\\')\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 10: Print Memory Footprint\n",
    "# ============================================================\n",
    "'''\n",
    "print(qt_model.get_memory_footprint())\n",
    "# Expected output: \"Model Memory Footprint: 5.3 MB\" (vs 21 MB for FP32)\n",
    "\n",
    "'''\n",
    "# ============================================================\n",
    "# STEP 11: Inference on Test Set\n",
    "# ============================================================\n",
    "'''\n",
    "qt_model.eval()\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for i, batch_data in enumerate(test_loader):\n",
    "        batch_data = train_utils.to_device(batch_data, device)\n",
    "        \n",
    "        # Run quantized inference\n",
    "        if opt.fusion_method == \\'intermediate\\':\n",
    "            pred_box_tensor, pred_score, gt_box_tensor, gt_label_tensor = \\\\\n",
    "                inference_utils_mc.inference_intermediate_fusion(\n",
    "                    batch_data,\n",
    "                    qt_model,  # Use quantized model!\n",
    "                    opencood_test_dataset\n",
    "                )\n",
    "        \n",
    "        # Calculate metrics (AP, etc.)\n",
    "        eval_utils_mc.calculate_tp_fp(...)\n",
    "\n",
    "print(\"Inference complete!\")\n",
    "```\n",
    "'''\n",
    "\n",
    "display(Markdown(ptq_workflow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-comparison'></a>\n",
    "### Model Structure Comparison: FP32 vs Quantized\n",
    "\n",
    "The `ic()` (IceCream) debugger is used in the PTQ script to print model structures.\n",
    "\n",
    "**Key Differences**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate model structure differences\n",
    "model_comparison = '''\n",
    "```python\n",
    "# Example output from ic(fp_model) and ic(qt_model)\n",
    "\n",
    "# ============================================================\n",
    "# FP32 Model Structure (fp_model)\n",
    "# ============================================================\n",
    "ic| fp_model: QuantModel(\n",
    "  (model): HeterPyramidCollabCodebookMC(\n",
    "    (pillar_vfe_m1): PillarVFE(\n",
    "      (pfn_layers): ModuleList(\n",
    "        (0): PFNLayer(\n",
    "          (linear): Linear(in_features=9, out_features=64, bias=False)\n",
    "          (norm): BatchNorm1d(64, ...)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (scatter_m1): PointPillarScatter()\n",
    "    (backbone_m1): ResNetBEVBackbone(\n",
    "      (blocks): Sequential(\n",
    "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        (1): BatchNorm2d(64, ...)\n",
    "        (2): ReLU(inplace=True)\n",
    "      )\n",
    "    )\n",
    "    (codebook): UMGMQuantizer(...)  # Codebook from Stage 3\n",
    "    (fusion_net): PyramidFusion(\n",
    "      (pyramid_blocks): ResNet(\n",
    "        (layer1): Conv2d(64, 64, ...)\n",
    "        (layer2): Conv2d(64, 128, ...)\n",
    "        (layer3): Conv2d(128, 256, ...)\n",
    "      )\n",
    "    )\n",
    "    (shrink_conv): DownsampleConv(\n",
    "      (conv): Conv2d(384, 256, kernel_size=(3, 3), padding=(1, 1))\n",
    "    )\n",
    "    (cls_head): Conv2d(256, 6, kernel_size=(1, 1))  # 3 classes Ã— 2 anchors\n",
    "    (reg_head): Conv2d(256, 42, kernel_size=(1, 1)) # 7 DOF Ã— 6\n",
    "    (dir_head): Conv2d(256, 12, kernel_size=(1, 1)) # 2 bins Ã— 6\n",
    "  )\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Quantized Model Structure (qt_model)\n",
    "# ============================================================\n",
    "ic| qt_model: QuantModel(\n",
    "  (model): HeterPyramidCollabCodebookMC(\n",
    "    (pillar_vfe_m1): QuantPillarVFE(  # â† Wrapped in Quant version\n",
    "      (pfn_layers): ModuleList(\n",
    "        (0): QuantPFNLayer(\n",
    "          (linear): QuantModule(  # â† Conv/Linear wrapped in QuantModule\n",
    "            (fwd_func): Linear(in_features=9, out_features=64, bias=False)\n",
    "            (weight_quantizer): UniformAffineQuantizer(\n",
    "              n_bits=8,\n",
    "              scale=Parameter[64],  # Per-channel scales\n",
    "              zero_point=Parameter[64]\n",
    "            )\n",
    "            (act_quantizer): UniformAffineQuantizer(\n",
    "              n_bits=8,\n",
    "              scale=Parameter[1],  # Per-layer scale\n",
    "              zero_point=Parameter[1]\n",
    "            )\n",
    "          )\n",
    "          # BatchNorm fused into previous QuantModule\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (scatter_m1): PointPillarScatter()\n",
    "    (backbone_m1): QuantResNetBEVBackbone(  # â† Quantized backbone\n",
    "      (blocks): Sequential(\n",
    "        (0): QuantModule(\n",
    "          (fwd_func): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "          (weight_quantizer): UniformAffineQuantizer(n_bits=8, ...)\n",
    "          (act_quantizer): UniformAffineQuantizer(n_bits=8, ...)\n",
    "        )\n",
    "        # BatchNorm and ReLU fused\n",
    "      )\n",
    "    )\n",
    "    (codebook): UMGMQuantizer(...)  # Kept in FP32 (small overhead)\n",
    "    (fusion_net): QuantPyramidFusion(\n",
    "      (pyramid_blocks): QuantResNet(\n",
    "        (layer1): QuantModule(Conv2d(64, 64, ...))\n",
    "        (layer2): QuantModule(Conv2d(64, 128, ...))\n",
    "        (layer3): QuantModule(Conv2d(128, 256, ...))\n",
    "      )\n",
    "    )\n",
    "    (shrink_conv): QuantDownsampleConv(\n",
    "      (conv): QuantModule(Conv2d(384, 256, ...))\n",
    "    )\n",
    "    # Detection heads: Quantization disabled (disable_network_output_quantization)\n",
    "    (cls_head): Conv2d(256, 6, kernel_size=(1, 1))  # FP32 for precision\n",
    "    (reg_head): Conv2d(256, 42, kernel_size=(1, 1)) # FP32\n",
    "    (dir_head): Conv2d(256, 12, kernel_size=(1, 1)) # FP32\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Layer Wrapping**:\n",
    "   - FP32: `Conv2d`, `Linear`\n",
    "   - Quantized: `QuantModule` wrapping `Conv2d`/`Linear`\n",
    "\n",
    "2. **Quantizers Added**:\n",
    "   - `weight_quantizer`: Per-channel for weights\n",
    "   - `act_quantizer`: Per-layer for activations\n",
    "   - Each has `scale` and `zero_point` parameters\n",
    "\n",
    "3. **BatchNorm Fusion**:\n",
    "   - FP32: Separate `BatchNorm2d` layers\n",
    "   - Quantized: BatchNorm fused into preceding `QuantModule`\n",
    "   - Reduces memory and improves efficiency\n",
    "\n",
    "4. **Codebook Preserved**:\n",
    "   - Codebook module kept in FP32 (negligible overhead)\n",
    "   - Focus PTQ on compute-heavy layers\n",
    "\n",
    "5. **Detection Heads**:\n",
    "   - Last 3 layers kept in FP32 for better accuracy\n",
    "   - Called via `disable_network_output_quantization()`\n",
    "'''\n",
    "\n",
    "display(Markdown(model_comparison))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quantmodel-impl'></a>\n",
    "### QuantModel Implementation\n",
    "\n",
    "The `QuantModel` class is the key to making a model quantizable. Let's see its **actual implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display QuantModel implementation\n",
    "quantmodel_code = '''\n",
    "```python\n",
    "# opencood/quant/quant_model.py (Actual implementation)\n",
    "\n",
    "import torch.nn as nn\n",
    "from opencood.quant.quant_block import specials, opencood_specials, BaseQuantBlock\n",
    "from opencood.quant.quant_layer import QuantModule, StraightThrough, UniformAffineQuantizer\n",
    "\n",
    "class QuantModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper that converts an FP32 model to a quantizable model.\n",
    "    \n",
    "    Recursively replaces:\n",
    "    - nn.Conv2d â†’ QuantModule(Conv2d)\n",
    "    - nn.Linear â†’ QuantModule(Linear)\n",
    "    - Custom blocks (PyramidFusion, etc.) â†’ QuantBlock versions\n",
    "    - Fuses BatchNorm into preceding layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, weight_quant_params: dict = {}, \n",
    "                 act_quant_params: dict = {}, is_fusing=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if is_fusing:\n",
    "            # Search and fuse BatchNorm layers\n",
    "            search_fold_and_remove_bn(model)\n",
    "            self.model = model\n",
    "            self.quant_module_refactor(self.model, weight_quant_params, act_quant_params)\n",
    "        else:\n",
    "            # Keep BatchNorm separate (for FP32 model)\n",
    "            self.model = model\n",
    "            self.quant_module_refactor_wo_fuse(self.model, weight_quant_params, act_quant_params)\n",
    "    \n",
    "    def quant_module_refactor(self, module: nn.Module, weight_quant_params: dict = {}, \n",
    "                              act_quant_params: dict = {}):\n",
    "        \"\"\"\n",
    "        Recursively replace layers with quantized versions.\n",
    "        \"\"\"\n",
    "        prev_quantmodule = None  # Track last QuantModule for BatchNorm fusion\n",
    "        \n",
    "        for name, child_module in module.named_children():\n",
    "            # Skip unquantized layers (e.g., codebook)\n",
    "            if name in specials_unquantized_names:\n",
    "                continue\n",
    "            \n",
    "            # Replace OpenCOOD-specific modules (PyramidFusion, etc.)\n",
    "            if type(child_module) in opencood_specials:\n",
    "                setattr(module, name, \n",
    "                       opencood_specials[type(child_module)](\n",
    "                           child_module, weight_quant_params, act_quant_params\n",
    "                       ))\n",
    "            \n",
    "            # Replace Conv2d and Linear\n",
    "            elif isinstance(child_module, (nn.Conv2d, nn.Linear)):\n",
    "                setattr(module, name, \n",
    "                       QuantModule(child_module, weight_quant_params, act_quant_params))\n",
    "                prev_quantmodule = getattr(module, name)\n",
    "            \n",
    "            # Fuse ReLU into previous QuantModule\n",
    "            elif isinstance(child_module, (nn.ReLU, nn.ReLU6)):\n",
    "                if prev_quantmodule is not None:\n",
    "                    prev_quantmodule.activation_function = child_module\n",
    "                    setattr(module, name, StraightThrough())  # Replace with passthrough\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            elif isinstance(child_module, StraightThrough):\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                # Recursively process sub-modules\n",
    "                self.quant_module_refactor(child_module, weight_quant_params, act_quant_params)\n",
    "    \n",
    "    def set_quant_state(self, weight_quant: bool = False, act_quant: bool = False):\n",
    "        \"\"\"\n",
    "        Enable/disable quantization for all QuantModules.\n",
    "        \n",
    "        Args:\n",
    "            weight_quant: Enable weight quantization\n",
    "            act_quant: Enable activation quantization\n",
    "        \"\"\"\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, (QuantModule, BaseQuantBlock)):\n",
    "                m.set_quant_state(weight_quant, act_quant)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "    def disable_network_output_quantization(self):\n",
    "        \"\"\"\n",
    "        Disable quantization for the last 3 layers (detection heads).\n",
    "        \n",
    "        This keeps classification, regression, and direction heads in FP32\n",
    "        for better precision in predictions.\n",
    "        \"\"\"\n",
    "        module_list = []\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, QuantModule):\n",
    "                module_list.append(m)\n",
    "        \n",
    "        if len(module_list) >= 3:\n",
    "            module_list[-1].disable_act_quant = True  # cls_head\n",
    "            module_list[-2].disable_act_quant = True  # reg_head\n",
    "            module_list[-3].disable_act_quant = True  # dir_head\n",
    "    \n",
    "```\n",
    "\n",
    "### Key Methods\n",
    "\n",
    "1. **`__init__`**: \n",
    "   - Wraps FP32 model\n",
    "   - Optionally fuses BatchNorm\n",
    "   - Calls `quant_module_refactor` to replace layers\n",
    "\n",
    "2. **`quant_module_refactor`**:\n",
    "   - Recursively traverses model\n",
    "   - Replaces Conv2d/Linear with QuantModule\n",
    "   - Replaces custom blocks with Quant versions\n",
    "   - Fuses BatchNorm and ReLU\n",
    "\n",
    "3. **`set_quant_state`**:\n",
    "   - Enable/disable quantization globally\n",
    "   - Used during calibration (False, False)\n",
    "   - Used during inference (True, True)\n",
    "\n",
    "4. **`disable_network_output_quantization`**:\n",
    "   - Keeps detection heads in FP32\n",
    "   - Improves prediction accuracy\n",
    "'''\n",
    "\n",
    "display(Markdown(quantmodel_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='part-iv'></a>\n",
    "# Part IV: Complete Workflow\n",
    "\n",
    "<a id='e2e'></a>\n",
    "## 11. End-to-End Training Workflow\n",
    "\n",
    "### Complete Pipeline (Start to Finish)\n",
    "\n",
    "```bash\n",
    "# ============================================================\n",
    "# STAGE 1: Full-Precision Pretraining\n",
    "# ============================================================\n",
    "python opencood/tools/train.py \\\n",
    "    -y opencood/hypes_yaml/v2x_real/Codebook/stage1/lidar_pyramid_stage1.yaml\n",
    "\n",
    "# Output: opencood/logs/stage1_model/net_epoch_best.pth\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 2: Codebook-Only Training\n",
    "# ============================================================\n",
    "python opencood/tools/train_stage2.py \\\n",
    "    --hypes_yaml opencood/hypes_yaml/v2x_real/Codebook/stage2/lidar_pyramid_stage2.yaml \\\n",
    "    --stage1_model opencood/logs/stage1_model/net_epoch_best.pth\n",
    "\n",
    "# Output: opencood/logs/stage2_model/net_epoch_best.pth\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 3: End-to-End Co-Training\n",
    "# ============================================================\n",
    "python opencood/tools/train_stage3.py \\\n",
    "    --hypes_yaml opencood/hypes_yaml/v2x_real/Codebook/stage3/lidar_pyramid_stage3.yaml \\\n",
    "    --stage2_model opencood/logs/stage2_model/net_epoch_best.pth\n",
    "\n",
    "# Output: opencood/logs/stage3_model/net_epoch_best.pth\n",
    "\n",
    "# ============================================================\n",
    "# INFERENCE: Full-Precision\n",
    "# ============================================================\n",
    "python opencood/tools/inference_mc.py \\\n",
    "    --model_dir opencood/logs/stage3_model \\\n",
    "    --fusion_method intermediate\n",
    "\n",
    "# ============================================================\n",
    "# PTQ: W8A8 Quantization\n",
    "# ============================================================\n",
    "python opencood/tools/inference_mc_quant.py \\\n",
    "    --model_dir opencood/logs/stage3_model \\\n",
    "    --fusion_method intermediate \\\n",
    "    --num_cali_batches 16 \\\n",
    "    --n_bits_w 8 \\\n",
    "    --n_bits_a 8 \\\n",
    "    --iters_w 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='summary'></a>\n",
    "## Summary\n",
    "\n",
    "This notebook provided a comprehensive guide to QuantV2X's 3-stage training pipeline and PTQ:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Stage 1**: Pretrain full-precision baseline (20 epochs, lr=0.002)\n",
    "2. **Stage 2**: Train codebook ONLY, freeze detector (20 epochs, lr=0.002)\n",
    "3. **Stage 3**: Co-train everything (10 epochs, lr=0.0002, batch=4)\n",
    "4. **PTQ**: W8A8 quantization for substantial speedup with <2% accuracy drop\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Paper**: [QuantV2X (arXiv:2509.03704)](https://arxiv.org/abs/2509.03704)\n",
    "- **Code**: [github.com/ucla-mobility/QuantV2X](https://github.com/ucla-mobility/QuantV2X)\n",
    "- **Dataset**: [V2X-Real](https://mobility-lab.seas.ucla.edu/v2x-real/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! ğŸš—âš¡**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
